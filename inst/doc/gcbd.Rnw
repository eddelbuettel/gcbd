
%% use JSS class but for now with nojss option
\documentclass[nojss,shortnames,article]{jss}
\usepackage{float}

<<echo=FALSE,print=FALSE>>=
options(width=50)
library(gcbd)
gcbd.version <- packageDescription("gcbd")$Version
gcbd.date <- packageDescription("gcbd")$Date
now.date <- strftime(Sys.Date(), "%B %d, %Y")
# dput(brewer.pal(5, "Set1"))
cols <- c("#E41A1C", "#377EB8", "#4DAF4A", "#984EA3", "#FF7F00")
# dput(brewer.pal(8,"Paired"))
paircols <- c("#A6CEE3", "#1F78B4", "#B2DF8A", "#33A02C", "#FB9A99", "#E31A1C", "#FDBF6F", "#FF7F00")
# create figures/ if not present
if ( ! (file.exists("figures") && file.info("figures")$isdir) ) dir.create("figures")
@
%

\author{Dirk Eddelbuettel\\Debian Project} % \And Second Author\\Plus Affiliation}
\title{Benchmarking single- and multi-core BLAS implementations and GPUs for use with \proglang{R}}

\Plainauthor{Dirk Eddelbuettel} % , Second Author} %% comma-separated
\Plaintitle{Benchmarking Single- and Multi-Core BLAS Implementaions and GPUs for Use with R}
\Shorttitle{Benchmarking BLAS and GPUs for Use with R}


\Abstract{
  \noindent
  We provide timing results for common linear algebra subroutines across BLAS
  and GPU-based implementations. Several BLAS (Basic Linear Algrebra
  Subprograms) implementations are compared. The first is the unoptimised
  reference BLAS which provides a baseline to measure against. Second is the
  Atlas tuned BLAS, configured for single-threaded mode. Third is the
  optimised and multi-threaded Goto BLAS. Fourth is the multi-threaded BLAS
  contained in the commercial Intel MKL package. We also measure the
  performance of two GPU-based implementations for \proglang{R} provided by
  the packages \pkg{gputools} as well as \pkg{magma}, a CPU/GPU hybrid.

  Several key computations are compared: matrix multiplication, QR
  decomposition, SVD decomposition and LU decomposition.

  While results are by their very nature dependent on the hardware aspects of
  the test platform, a few general lessons can be drawn.
}
\Keywords{BLAS, Atlas, Goto, MKL, GPU, \proglang{R}} %% at least one keyword must be supplied
\Plainkeywords{BLAS, Atlas, Goto, MKL, GPU, R} %% without formatting

%% publication information
%% NOTE: Typically, this can be left commented and will be filled out by the technical editor
%% \Volume{13}
%% \Issue{9}
%% \Month{September}
%% \Year{2004}
%% \Submitdate{2004-09-29}
%% \Acceptdate{2004-09-29}

\Address{
  Dirk Eddelbuettel \\
  Debian Project \\
  River Forest, IL, USA\\
  %% Telephone: +43/1/31336-5053
  %% Fax: +43/1/31336-734
  E-mail: \email{edd@debian.org}\\
  URL: \url{http://dirk.eddelbuettel.com}
}

%% need no \usepackage{Sweave.sty}


% ------------------------------------------------------------------------

\begin{document}
\SweaveOpts{engine=R,eps=FALSE,echo=FALSE,prefix.string=figures/chart}
%\VignetteIndexEntry{BLAS and GPU Benchmarking}
%\VignetteDepends{gputools,magma
%\VignetteKeywords{blas,gpu,atlas,mkl,goto}
%\VignettePackage{gcbd}

\shortcites{Brodtkorb_et_al_2010,Blackford_et_al:2002,lapack}

%% Note that you should use the \pkg{}, \proglang{} and \code{} commands.

\begin{quote}
  This version corresponds to \pkg{gcbd} \Sexpr{gcbd.version} and was
  compiled on \Sexpr{now.date}.

  \textbf{This is an incomplete and early version with possibly premature
    results. Please do not circulate or cite yet.}
\end{quote}

\section[Introduction]{Introduction}

Analysts are often eager to reap the maximum performance from their computing
platform.  A popular suggestion in recent years has been to consider
optimised algebra subprograms (BLAS).  Some of these optimised BLAS
libraries have been included with commercial analysis platforms for a decade
\citep{Moler:2000}. Some have also been available for some Linux
distributions for an equally long or even longer time
\citep{Maguire:1999}.  Just to consider another example, the \proglang{R}
language and environment devotes a detailed discussion to the topic in its
\textsl{Installation and   Administration} manual
\citep[appendix A.3.1]{RCore:InstAdmin}.

Among the BLAS implementations, several popular choices have emerged. Atlas
(an acronym for \textsl{automatically tuned linear algebra system}) is
popular as it has shown very good performance due to the automated and
cpu-specific tuning it offers \citep{Whaley_Dongarra:1999,
  Whaley_Petitet:2005}. It is also licensed in such a way that it permits
distributors to include it in their products.  Another popular BLAS
implementation is Goto BLAS which is named after its main developer,
Kazushige Goto \citep{Goto_VanDeGeijin:2008}. While `free to use', its
license does not permit redistribution.  Lastly, the Intel Math Kernel
Library (MKL), a commercial product, also includes an optimised BLAS library.

A recent addition to the toolchain of high-performance computing are
graphical processing units (GPUs).  Originally designed for optimised
single-precision arithmetic to accelerate computing as performed by graphics
cards, these devices are increasingly used in numerical analysis.  Earlier
criticism of insufficient floating-point precisions or severe performance
penalties for double-precision calculation are being addressed by the newest
models. Dependence on particular vendors remains a concern with NVidia's CUDA
toolkit \citep{nVidia:2010} currently still the development choice where the
newer OpenCL standard \citep{OpenCL:2010} may become a more generic
successor. \citet{Brodtkorb_et_al_2010} provide an excellent recent
survey.

But what has been lacking is a comparison of the effective performance of these
alternative.  This paper works towards answering this question.  By comparing
performance across four different BLAS implementations---as well as two
GPU-based solutions---we are able to provide a reasonably broad comparison.

The rest of the paper is organised as follows. In the next section, the
technical background is discussed. The implementation of our approach
is outlined in section 3. We provide our results in the section 4. A summary
concludes.

\section{Background}

Basic Linear Algrebra Subprograms (BLAS) provide an Application Programming
Interface (API) for linear algebra.  For a given task as, say, a
multiplication of two conformant matrices, an interface is described via a
function declaration. The actual implementation becomes interchangeable
thanks to the API definition and can be supplied by different implementations
or algorithms.  This is one of the fundamental code design features we are
using here to benchmark the difference in performance from different
implementations.

A second key aspect is the difference between static and shared linking.  In
static linking, object code is copied from the underlying library and copied
into the resulting executable.  This has several key implications. First, the
executable becomes larger due to the copy of the binary code. Second, it makes it
marginally faster as the library code is present and no additional lookup and
subsequent redirection has to be performed (and the amount of this performance
penalty is the subject of near endless debate). Third, it makes the program
more robust as fewer external dependencies are required.  However, this last
point also has a downside: no changes in the underlying library will be
reflected in the binary unless a new build is executed.  Shared library
builds, on the other hand, result in smaller binaries that may run marginally
slower -- but can make use of different libraries without a rebuild.  That
last feature is key here.

Because of both the standardised interface of the BLAS, and the fact that we
have several alternative implementations at our disposal, we can readily
switch between these alternatives. This makes use of a package mechanism
used by the Linux distribution we employ. However, a simpler (yet less
robust) approach would also be available. This technical aspect is discussed
further below.

The first available BLAS implementation stems from the original unoptimised
code on Netlib. On Debian-based systems, it is provided by the (source)
package \pkg{blas} \citep{Blackford_et_al:2002} and used with the
\pkg{lapack} package \citep{lapack}. This implementation is commonly referred
to as 'reference BLAS' (and we will use 'ref' as a shorthand) as it provides
a refernce implementation.

The second BLAS implementation is provided by Atlas
\citep{Whaley_Dongarra:1999,Whaley_Petitet:2005}. Its names stands for
\textsl{automatically tuned linear algebra software} as it optimises its
performance and parameters (such as cache sizes) during its initial build.
Another notable aspect is its liberal licensing which permits wide
distribution. Consequently, Atlas is available on most if not all Linux
distributions. Windows binaries are also widely available.

The third available BLAS implementation is part of the Intel Math Kernel
Library (MKL), a commercial product. However, Ubuntu release 9.10
(``karmic'') contains a set of packages sponsored by Revolution Analytics
which comprises the Intel MKL in a setup directly useable by \proglang{R}. We
will use these packages here as a first set of optimised BLAS.

The fourth BLAS implementation is provided by the Goto BLAS. Upon the
required user registration, these are freely available from the University of
Texas, albeit under a license that prohibits redistribution.  This prevents
inclusion into popular Liux distributions, a clear disadvantage for easy
installation and de-installation.  However, the contributed Debian repository
at the Institute for Statistical Mathematics in Japan provides a `helper
package' \citep{Nakama:2010} which, given the required registration
information, downloads and compiles the Goto BLAS in such a manner that
another binary Debian package is produced.  This permits us to use the Goto
BLAS via the resulting package as a third BLAS alternative.

The first \proglang{R} extension for graphics-processing units has been implemented
by the \pkg{gputools} package \citep*{cran:gputools}. It provides a number of
functions which use the GPU instead of the CPU which results in a significant
performance increase for `large enough' problems.  Because data has to be transferred
from the CPU to the GPU, a fixed cost in communications has to be borne by
every invocation of the GPU.  For sizeable problems, this cost can be
outweighed by the benefits of the massively parallel GPU computation.
Exactly where the indifference point lies beyond which GPU computing has an
advantage is unfortunately dependent on the particular problem and algorithm
as well as the given hardware and software combination.

A recent addition to the set of \proglang{R} packages is \pkg{magma}
\citep{cran:magma} which interfaces the \pkg{Magma} library project of the
same name \citep{Tomov_et_al:2009,Tomov_et_al:2010} (where we will capitalize the
name of the library to distinguish it from the \proglang{R} package). The
stated goal of the Magma project it to \textsl{develop a dense linear algebra
  library similar to LAPACK but for heterogeneous/hybrid architectures,
  starting with current "Multicore+GPU" systems}.
The current release of Magma is version 0.2 which is available for
Linux systems with NVIDIA's CUDA toolkit, and provides
LU, QR, and Cholesky factorizations as well as linear solvers based on these
along with implementations pf several BLAS function including gemm, gemv,
symv, and trsm.
The promise

A flexible and hybrid approach has the potential to improve upon solutions
that use either the CPU or the GPU. To the best of our knowledge, this paper
offers the first benchmark comparison of Magma versus BLAS implementations.


\section{Implementation}

\subsection{Requirements}

In order to undertake the automated benchmarking, we need to be able to
switch between different implementations of the BLAS API.  As discussed
above, dynamic libraries are one possible solution that avoids having to
rebuild \proglang{R} explicitly for each library.  However, this also
requires that \proglang{R} itself is built with shared-library support, as
well as with support for external BLAS and LAPACK libraries.  This is the
default on Debian and Ubuntu system.

The reference BLAS as well as Atlas have been available for essentially all
Debian and Ubuntu releases.  The Intel MKL is available for Ubuntu following
the upload for release 9.10.

For Goto BLAS, we are making use of a helper script provided in the
contributed \texttt{gotoblas2-helper} package \citep{Nakama:2010}. This
package arranges for a download of the Goto BLAS sources (provided the
required account and password information for the University of Texas
software download center) and automated Debian package build and installation
via the command \code{sudo /etc/init.d/gotoblas2-helper start}. Note the
initial invocation of this command will trigger a build of the package which
may take up to two hours.  While designed for Debian, it also works perfectly
on our Ubuntu systems.\footnote{The \texttt{/etc/init.d/gotoblas2-helper}
  script required one change from \texttt{/bin/sh} to \texttt{/bin/bash}.}

For GPU-based testing we require the \proglang{R} packages \pkg{gputools} and
\pkg{magma} which in turn require support for CUDA and the NVIDIA
SDK. Detailled installation instructions are provided by either package so we
will defer to these and assume that the packages are in fact installed.
Helper scripts will then verify this while the benchmark is executed.

\subsection{Benchmark Implementation}

The bechmarks described in this paper are produced by the package \pkg{gcbd}
which is part of the larger \pkg{gcb} package on R-Forge
\citep*{rforge:gcb}. The \pkg{gcbd} package (where the acronym expands to
GPU/CPU Benchmarking on .deb-based systems) contains a number of \proglang{R}
helper functions as well as an actual benchmarking script which is executed.

The helper functions fall into two groups: utilities, and benchmarks. The
utilities fall into several categories:
\begin{itemize}
\item feature detection via a function \code{requirements()} which asserts
  that a number of testable features of the host operating system are met;
\item feature tests via functions \code{hasMagma()} and \code{hasGputools()}
  allowing the benchmark script to bypass GPU-based tests in systems without
  a GPU;
\item installation and removal functions which interface the package
  management layer and install (or remove) the Atlas, MKL or Goto BLAS
  packages, respectively, which helps ensure that at any one point in time
  only one accelerated BLAS library package is present;
\item database creation where the results database is created if not already present;
\item recording of simulation results in the database.
\end{itemize}

The benchmark functions can also be categorized:
\begin{itemize}
\item random data creation for standard \pkg{Matrix} or \pkg{magma} objects;
\item actual benchmarking code for
  \begin{itemize}
  \item matrix crossproducs;
  \item SVD decomposition;
  \item QR decomposition;
  \item LU decomposition;
  \end{itemize}
  for BLAS, \pkg{gputools} and \pkg{magma}, respectively;
\end{itemize}

\subsection{Benchmark script}

The actual benchmark is then provided by the script \code{benchmark.r} in the
subdirectory code{scripts} of the \pkg{gcbd} package.

It is implemented an executable \proglang{R} script which uses the
\pkg{getopt} package for command-line parsing:

\begin{verbatim}
$ ./benchmark.r -h
Usage: benchmark.r [-[-verbose|v]] [-[-help|h]] [-[-nobs|n] <integer>]
                   [-[-runs|r] <integer>] [-[-benchmark|b] <character>]
    -v|--verbose      verbose operations, default is false
    -h|--help         help on options
    -n|--nobs         number of rows and columns in matrix, default is 250
    -r|--runs         number of benchmark runs, default is 30
    -b|--benchmark    benchmark to run (matmult, qr, svd, lu), default is matmult
$
\end{verbatim}

Each benchmark experiments attempt to run $r$ runs for a matrix of size $n
\times n$ observations using the chosen benchmark---one of matrix
crossproduct or one of the QR, LU or SVD decompositions---over all four BLAS
implementations and two GPU packages.  The run against the GPU package is
optional and depdendent on the GPU packages being present.

At the end of each benchmark experiment, the results are appended to SQLite
database.

We use the elapsed time as measured by the \proglang{R} function
\code{system.time()}.  This measure is preferable over the sum of system time
and user time which add up total cputime---but without adjusting for multiple
threads or cores. Elapsed time correctly measures from start to finish,
whether one or multiple threads or core are involved, but is susceptible to
be influenced by system load which is a potential shortcoming.


\subsection{Alternative implementation}

On platforms that do not have access to pre-built BLAS library packages, an
alternative approach could consist of locally installing the different
libraries into subdirectories of, say, \code{/opt/blas}. One could then use
the environment variable \verb|LD_LIBRARY_PATH| to select one of these
directories at a time.  However, such an approach places a larger burden on
the user of the benchmarking software as she would have to install the BLAS
libraries which is typically not a trivial step.

\subsection{Hardware consideration}

Hardware aspects matter greatly for the overall results. Different CPU will
generate different performance profiles.

We have been running the results presented here on these two platforms:
\begin{itemize}
\item a four-core Intel i7 920 (2.67 GHz) in hyperthreaded mode for eight
  visible cores, running Ubuntu 10.4 in 64-bit mode;
\item a dual-cpu four-core Intel Xeon 5570 (2.96 Ghz) in hyperthreaded mode
  for sixteen cores, running Ubuntu 10.4 in 64-bit mode;
\end{itemize}

The i7 system has a NVIDIA Quadro FX4800 GPU with 192 cores. This GPU card is
the only one we have tested the benchmarks against.

Different hardware platforms could be reflected by other users installing the
\pkg{gcbd} package on their system and reporting results back by supplying
the resulting SQLite database files.

%\pagebreak
\section[Results]{Results}

<<print=FALSE>>=
dbcon <- dbConnect(dbDriver("SQLite"), dbname=system.file("sql", "gcbd.sqlite", package="gcbd"))
i7 <- dbGetQuery(dbcon, 'select * from benchmark where host="max" order by nobs')
xeon <- dbGetQuery(dbcon, 'select * from benchmark where host="xeon_X5570" order by nobs')
invisible(dbDisconnect(dbcon))
D <- subset(i7[,-c(1:2,5)], type=="matmult")
@

\subsection{BLAS Comparison}

We present the benchmark results (which are also included in the \pkg{gcbd}
package in the file \code{sql/gcbd.sqlite}).  We will shown one type of
benchmark per test architecture (currently: i7 or xeon) which times first
shown on a regular scale and also on a log-scale.

\subsubsection{Matrix Multiplication: i7 with GPU}

\setkeys{Gin}{width=0.99\textwidth}
\begin{figure}[H]
  \centering
<<fig=TRUE,height=6,width=12>>=
D <- subset(i7[,-c(1:2,5)], type=='matmult')
op <- par(mfrow=c(1,2))
matplot(x=D[,"nobs"], y=D[,c("ref","atlas","mkl","gotob","gpu")], type='l', lty=1, col=cols,
        xlab="Matrix dimension", ylab="Time in seconds", main="Matrix Multiplication")
legend("topleft", legend=c("Ref","Atlas","MKL","Goto","GPU"), bty="n", col=cols, lty=1)
matplot(x=D[,"nobs"], y=D[,c("ref","atlas","mkl","gotob","gpu")], type='l', lty=1, col=cols,
        xlab="Matrix dimension", ylab="Time in seconds (in logs)", main="Matrix Multiplication", log="y")
legend("bottomright", legend=c("Ref","Atlas","MKL","Goto","GPU"), bty="n", col=cols, lty=1)
par(op)
@
\end{figure}

It is immediately apparent that the reference BLAS underperform strongly. The
single-thread Atlas performance is also dominated by the multi-threaded BLAS
(Goto, MKL) and the GPU-based gputools.  We also see (more clearly in logs)
that the GPU-based solution bears a clear penalty for smaller dimension and
becomes close to being competetive with MKL and Goto without actually
surpassing them on the chosen matrix sizes.

\subsubsection{Matrix Multiplication: Xeon}

\begin{figure}[H]
  \centering
<<fig=TRUE,height=6,width=12>>=
D <- subset(xeon[,-c(1:2,5)], type=='matmult')
op <- par(mfrow=c(1,2))
matplot(x=D[,"nobs"], y=D[,c("ref","atlas","mkl","gotob")], type='l', lty=1, col=cols,
        xlab="Matrix dimension", ylab="Time in seconds", main="Matrix Multiplication")
legend("topleft", legend=c("Ref","Atlas","MKL","Goto"), bty="n", col=cols, lty=1)
matplot(x=D[,"nobs"], y=D[,c("ref","atlas","mkl","gotob")], type='l', lty=1, col=cols,
        xlab="Matrix dimension", ylab="Time in seconds (in logs)", main="Matrix Multiplication", log="y")
legend("bottomright", legend=c("Ref","Atlas","MKL","Goto"), bty="n", col=cols, lty=1)
par(op)
@
\end{figure}

This benchmark offers a clearer differentiation between the multi-threaded
BLAS with Goto outperforming the Intel MKL. [FIXME/REDO ?] The Intel MKL is
also seen as having an erratic performance which may call for redo.


\subsubsection{QR Decomposition: i7 with GPU}

\begin{figure}[H]
  \centering
<<fig=TRUE,height=6,width=12>>=
D <- subset(i7[,-c(1:2,5)], type=='qr')
op <- par(mfrow=c(1,2))
matplot(x=D[,"nobs"], y=D[,c("ref","atlas","mkl","gotob","gpu")], type='l', lty=1, col=cols,
        xlab="Matrix dimension", ylab="Time in seconds", main="QR Decomposition")
legend("topleft", legend=c("Ref","Atlas","MKL","Goto","GPU"), bty="n", col=cols, lty=1)
matplot(x=D[,"nobs"], y=D[,c("ref","atlas","mkl","gotob","gpu")], type='l', lty=1, col=cols, log="y",
        xlab="Matrix dimension", ylab="Time in seconds (in logs)", main="QR Decomposition")
legend("bottomright", legend=c("Ref","Atlas","MKL","Goto","GPU"), bty="n", col=cols, lty=1)
par(op)
@
\end{figure}

This benchmarks offers a first glimpse at the potential for GPU. For
matrix dimensions in excess of 1500 to 2000, the \pkg{gputools}-based
solution dominates.  Interetsingly, the Intel MKL also outperformns the Goto BLAS.


\subsubsection{QR Decomposition: Xeon}

\begin{figure}[H]
  \centering
<<fig=TRUE,height=6,width=12>>=
D <- subset(xeon[,-c(1:2,5)], type=='qr')
op <- par(mfrow=c(1,2))
matplot(x=D[,"nobs"], y=D[,c("ref","atlas","mkl","gotob")], type='l', lty=1, col=cols,
        xlab="Matrix dimension", ylab="Time in seconds", main="QR Decomposition")
legend("topleft", legend=c("Ref","Atlas","MKL","Goto"), bty="n", col=cols, lty=1)
matplot(x=D[,"nobs"], y=D[,c("ref","atlas","mkl","gotob")], type='l', lty=1, col=cols,
        xlab="Matrix dimension", ylab="Time in seconds (in logs)", main="QR Decomposition", log="y")
legend("bottomright", legend=c("Ref","Atlas","MKL","Goto"), bty="n", col=cols, lty=1)
par(op)
@
\end{figure}

The result seen on the i7 platform is confirmed here: Intel MKL has an
advantage over Goto for QR decompositions. However, it is surprising that the
Goto performance is also dominated by the reference BLAS and
Atlas. [FIXME/REDO: Check!]

\subsubsection{SVD Decomposition: i7 with GPU}

\begin{figure}[H]
  \centering
<<fig=TRUE,height=6,width=12>>=
D <- subset(i7[,-c(1:2,5)], type=='svd')
op <- par(mfrow=c(1,2))
matplot(x=D[,"nobs"], y=D[,c("ref","atlas","mkl","gotob","gpu")], type='l', lty=1, col=cols,
        xlab="Matrix dimension", ylab="Time in seconds", main="SVD Decomposition")
legend("topleft", legend=c("Ref","Atlas","MKL","Goto","GPU"), bty="n", col=cols, lty=1)
matplot(x=D[,"nobs"], y=D[,c("ref","atlas","mkl","gotob","gpu")], type='l', lty=1, col=cols, log="y",
        xlab="Matrix dimension", ylab="Time in seconds (in logs)", main="SVD Decomposition")
legend("bottomright", legend=c("Ref","Atlas","MKL","Goto","GPU"), bty="n", col=cols, lty=1)
par(op)
@
\end{figure}

This outcome is similar to matrix cross-product: the two multi-threaded BLAS
are indistinguishable, and the GPU-based solution is becoming competitive for
large matrix sizes.

\subsubsection{SVD Decomposition: Xeon}

\begin{figure}[H]
  \centering
<<fig=TRUE,height=6,width=12>>=
D <- subset(xeon[,-c(1:2,5)], type=='svd')
op <- par(mfrow=c(1,2))
matplot(x=D[,"nobs"], y=D[,c("ref","atlas","mkl","gotob")], type='l', lty=1, col=cols,
        xlab="Matrix dimension", ylab="Time in seconds", main="SVD Decomposition")
legend("topleft", legend=c("Ref","Atlas","MKL","Goto"), bty="n", col=cols, lty=1)
matplot(x=D[,"nobs"], y=D[,c("ref","atlas","mkl","gotob")], type='l', lty=1, col=cols,
        xlab="Matrix dimension", ylab="Time in seconds (in logs)", main="SVD Decomposition", log="y")
legend("bottomright", legend=c("Ref","Atlas","MKL","Goto"), bty="n", col=cols, lty=1)
par(op)
@
\end{figure}

On the Xeon, we see Goto BLAS once again dominate the MKL BLAS whose
performance is seen as little erratice.


\subsubsection{LU Decomposition: i7 with GPU}

\begin{figure}[H]
  \centering
<<fig=TRUE,height=6,width=12>>=
D <- subset(i7[,-c(1:2,5)], type=='lu')
op <- par(mfrow=c(1,2))
matplot(x=D[,"nobs"], y=D[,c("ref","atlas","mkl","gotob")], type='l', lty=1, col=cols,
        xlab="Matrix dimension", ylab="Time in seconds", main="LU Decomposition")
legend("topleft", legend=c("Ref","Atlas","MKL","Goto"), bty="n", col=cols, lty=1)
matplot(x=D[,"nobs"], y=D[,c("ref","atlas","mkl","gotob")], type='l', lty=1, col=cols, log="y",
        xlab="Matrix dimension", ylab="Time in seconds (in logs)", main="LU Decomposition")
legend("bottomright", legend=c("Ref","Atlas","MKL","Goto"), bty="n", col=cols, lty=1)
par(op)
@
\end{figure}

No LU on GPU with \pkg{gputools}


\subsubsection{LU Decomposition: Xeon}

\begin{figure}[H]
  \centering
<<fig=TRUE,height=6,width=12>>=
D <- subset(xeon[,-c(1:2,5)], type=='lu')
op <- par(mfrow=c(1,2))
matplot(x=D[,"nobs"], y=D[,c("ref","atlas","mkl","gotob")], type='l', lty=1, col=cols,
        xlab="Matrix dimension", ylab="Time in seconds", main="LU Decomposition")
legend("topleft", legend=c("Ref","Atlas","MKL","Goto"), bty="n", col=cols, lty=1)
matplot(x=D[,"nobs"], y=D[,c("ref","atlas","mkl","gotob")], type='l', lty=1, col=cols,
        xlab="Matrix dimension", ylab="Time in seconds (in logs)", main="LU Decomposition", log="y")
legend("bottomright", legend=c("Ref","Atlas","MKL","Goto"), bty="n", col=cols, lty=1)
par(op)
@
\end{figure}


\subsection{Magma: GPU and BLAS combined}

\subsubsection{Direct usage and comparison to BLAS}

\begin{figure}[H]
  \centering
<<fig=TRUE,height=6,width=12>>=
D <- subset(i7[,-c(1:2,5)], type=='matmult')
op <- par(mfrow=c(1,2))
matplot(x=D[,"nobs"], y=D[,c("atlas","magmaAtlas","gotob","magmaGoto", "mkl", "magmaMkl")],
        type='l', lty=1, col=paircols,
        xlab="Matrix dimension", ylab="Time in seconds", main="Matrix Multiplication")
legend("topleft", legend=c("Atlas","Atlas/Magma", "Goto", "Goto/Magma", "MKL", "MKL/Magma"),
       bty="n", col=paircols, lty=1)
matplot(x=D[,"nobs"], y=D[,c("atlas","magmaAtlas","gotob","magmaGoto", "mkl", "magmaMkl")],
        type='l', lty=1, col=paircols, log="y",
        xlab="Matrix dimension", ylab="Time in seconds (in logs)", main="Matrix Multiplication")
legend("bottomright",
       legend=c("Atlas","Atlas/Magma", "Goto", "Goto/Magma", "MKL", "MKL/Magma"),
       bty="n", col=paircols, lty=1)
par(op)
@
\end{figure}

\begin{figure}[H]
  \centering
<<fig=TRUE,height=6,width=12>>=
D <- subset(i7[,-c(1:2,5)], type=='qr')
op <- par(mfrow=c(1,2))
matplot(x=D[,"nobs"], y=D[,c("atlas","magmaAtlas","gotob","magmaGoto", "mkl", "magmaMkl")],
        type='l', lty=1, col=paircols,
        xlab="Matrix dimension", ylab="Time in seconds", main="QR Decomposition")
legend("topleft", legend=c("Atlas","Atlas/Magma", "Goto", "Goto/Magma", "MKL", "MKL/Magma"),
       bty="n", col=paircols, lty=1)
matplot(x=D[,"nobs"], y=D[,c("atlas","magmaAtlas","gotob","magmaGoto", "mkl", "magmaMkl")],
        type='l', lty=1, col=paircols, log="y",
        xlab="Matrix dimension", ylab="Time in seconds (in logs)", main="QR Decomposition")
legend("bottomright",
       legend=c("Atlas","Atlas/Magma", "Goto", "Goto/Magma", "MKL", "MKL/Magma"),
       bty="n", col=paircols, lty=1)
par(op)
@
\end{figure}

\begin{figure}[H]
  \centering
<<fig=TRUE,height=6,width=12>>=
D <- subset(i7[,-c(1:2,5)], type=='svd')
op <- par(mfrow=c(1,2))
matplot(x=D[,"nobs"], y=D[,c("atlas","magmaAtlas","gotob","magmaGoto", "mkl", "magmaMkl")],
        type='l', lty=1, col=paircols,
        xlab="Matrix dimension", ylab="Time in seconds", main="SVD Decomposition")
legend("topleft", legend=c("Atlas","Atlas/Magma", "Goto", "Goto/Magma", "MKL", "MKL/Magma"),
       bty="n", col=paircols, lty=1)
matplot(x=D[,"nobs"], y=D[,c("atlas","magmaAtlas","gotob","magmaGoto", "mkl", "magmaMkl")],
        type='l', lty=1, col=paircols, log="y",
        xlab="Matrix dimension", ylab="Time in seconds (in logs)", main="SVD Decomposition")
legend("bottomright",
       legend=c("Atlas","Atlas/Magma", "Goto", "Goto/Magma", "MKL", "MKL/Magma"),
       bty="n", col=paircols, lty=1)
par(op)
@
\end{figure}

\subsubsection{Performance relative to reference BLAS}

\begin{figure}[H]
  \centering
<<fig=TRUE,height=6,width=12>>=
op <- par(mfrow=c(1,2))
D <- subset(i7[,-c(1:2,5)], type=='matmult')
N <- cbind(D[,"nobs"],
           D[,"ref"]/D[,"atlas"],
           D[,"ref"]/D[,"magmaAtlas"],
           D[,"ref"]/D[,"gotob"],
           D[,"ref"]/D[,"magmaGoto"],
           D[,"ref"]/D[,"mkl"],
           D[,"ref"]/D[,"magmaMkl"],
           D[,"ref"]/D[,"gpu"])
matplot(x=D[,"nobs"], y=N[,-1],
        type='l', lty=1, col=paircols, #pch=".",
        xlab="Matrix dimension", ylab="Performance relative to Ref.BLAS",
        main="Matrix Multiplication:\nRatio of Reference BLAS to given BLAS")
legend("bottomright",
       legend=c("Atlas","Atlas/Magma", "Goto", "Goto/Magma", "MKL", "MKL/Magma", "GPU"),
       bty="n", col=paircols, lty=1)

D <- subset(i7[,-c(1:2,5)], type=='qr')
N <- cbind(D[,"nobs"],
           D[,"ref"]/D[,"atlas"],
           D[,"ref"]/D[,"magmaAtlas"],
           D[,"ref"]/D[,"gotob"],
           D[,"ref"]/D[,"magmaGoto"],
           D[,"ref"]/D[,"mkl"],
           D[,"ref"]/D[,"magmaMkl"],
           D[,"ref"]/D[,"gpu"])
matplot(x=D[,"nobs"], y=N[,-1],
        type='l', lty=1, col=paircols, #pch=".",
        xlab="Matrix dimension", ylab="Performance relative to Ref.BLAS",
        main="QR Decomposition:\nRatio of Reference BLAS to given BLAS")
legend("bottomright",
       legend=c("Atlas","Atlas/Magma", "Goto", "Goto/Magma", "MKL", "MKL/Magma", "GPU"),
       bty="n", col=paircols, lty=1)
@
\end{figure}


\begin{figure}[H]
  \centering
<<fig=TRUE,height=6,width=12>>=
op <- par(mfrow=c(1,2))
D <- subset(i7[,-c(1:2,5)], type=='svd')
N <- cbind(D[,"nobs"],
           D[,"ref"]/D[,"atlas"],
           D[,"ref"]/D[,"magmaAtlas"],
           D[,"ref"]/D[,"gotob"],
           D[,"ref"]/D[,"magmaGoto"],
           D[,"ref"]/D[,"mkl"],
           D[,"ref"]/D[,"magmaMkl"],
           D[,"ref"]/D[,"gpu"])
matplot(x=D[,"nobs"], y=N[,-1],
        type='l', lty=1, col=paircols, #pch=".",
        xlab="Matrix dimension", ylab="Performance relative to Ref.BLAS",
        main="SVD Decomposition:\nRatio of Reference BLAS to given BLAS")
legend("bottomright",
       legend=c("Atlas","Atlas/Magma", "Goto", "Goto/Magma", "MKL", "MKL/Magma", "GPU"),
       bty="n", col=paircols, lty=1)

D <- subset(i7[,-c(1:2,5)], type=='lu' & nobs<=2000)
N <- cbind(D[,"nobs"],
           D[,"ref"]/D[,"atlas"],
           D[,"ref"]/D[,"magmaAtlas"],
           D[,"ref"]/D[,"gotob"],
           D[,"ref"]/D[,"magmaGoto"],
           D[,"ref"]/D[,"mkl"],
           D[,"ref"]/D[,"magmaMkl"])
matplot(x=D[,"nobs"], y=N[,-1],
        type='l', lty=1, col=paircols, #pch=".",
        xlim=c(min(D[,"nobs"]), 4000),  # fill in for missing
        xlab="Matrix dimension", ylab="Performance relative to Ref.BLAS",
        main="LU Decomposition:\nRatio of Reference BLAS to given BLAS")
legend("bottomright",
       legend=c("Atlas","Atlas/Magma", "Goto", "Goto/Magma", "MKL", "MKL/Magma"),
       bty="n", col=paircols, lty=1)
@
\end{figure}

\subsubsection{Performance of magma and BLAS relative to BLAS}

\begin{figure}[H]
  \centering
<<fig=TRUE,height=6,width=12>>=
op <- par(mfrow=c(1,2))
D <- subset(i7[,-c(1:2,5)], type=='matmult')
N <- cbind(D[,"nobs"],
           D[,"atlas"]/D[,"magmaAtlas"],
           D[,"gotob"]/D[,"magmaGoto"],
           D[,"mkl"]/D[,"magmaMkl"])
matplot(x=D[,"nobs"], y=N[,-1],
        type='l', lty=1, col=cols[-1], #pch=".",
        xlab="Matrix dimension", ylab="Magma performance relative to BLAS",
        main="Matrix Multiplication:\nRatio of BLAS to BLAS+Magma")
legend("bottomright", legend=c("Atlas","Goto", "MKL"),
       bty="n", col=cols[-1], lty=1)

D <- subset(i7[,-c(1:2,5)], type=='qr')
N <- cbind(D[,"nobs"],
           D[,"atlas"]/D[,"magmaAtlas"],
           D[,"gotob"]/D[,"magmaGoto"],
           D[,"mkl"]/D[,"magmaMkl"])
matplot(x=D[,"nobs"], y=N[,-1],
        type='l', lty=1, col=cols[-1], #pch=".",
        xlab="Matrix dimension", ylab="Magma performance relative to BLAS",
        main="QR Decomposition:\nRatio of BLAS to BLAS+Magma")
legend("bottomright", legend=c("Atlas","Goto", "MKL"),
       bty="n", col=cols[-1], lty=1)
@
\end{figure}


\begin{figure}[H]
  \centering
<<fig=TRUE,height=6,width=12>>=
op <- par(mfrow=c(1,2))
D <- subset(i7[,-c(1:2,5)], type=='svd')
N <- cbind(D[,"nobs"],
           D[,"atlas"]/D[,"magmaAtlas"],
           D[,"gotob"]/D[,"magmaGoto"],
           D[,"mkl"]/D[,"magmaMkl"])
matplot(x=D[,"nobs"], y=N[,-1],
        type='l', lty=1, col=cols[-1], #pch=".",
        xlab="Matrix dimension", ylab="Magma performance relative to BLAS",
        main="SVD Decomposition:\nRatio of BLAS to BLAS+Magma")
legend("bottomright", legend=c("Atlas", "Goto", "MKL"),
       bty="n", col=cols[-1], lty=1)

D <- subset(i7[,-c(1:2,5)], type=='lu' & nobs<=2000)
N <- cbind(D[,"nobs"],
           D[,"atlas"]/D[,"magmaAtlas"],
           D[,"gotob"]/D[,"magmaGoto"],
           D[,"mkl"]/D[,"magmaMkl"])
matplot(x=D[,"nobs"], y=N[,-1],
        type='l', lty=1, col=cols[-1], #pch=".",
        xlim=c(min(D[,"nobs"]), 4000),  # fill in for missing
        xlab="Matrix dimension", ylab="Magma oerformance relative to BLAS",
        main="LU Decomposition:\nRatio of BLAS to BLAS+Magma")
legend("bottomright", legend=c("Atlas", "Goto", "MKL"),
       bty="n", col=cols[-1], lty=1)
@
\end{figure}


\section[Summary]{Summary}

TBD

\bibliography{gcbd}

\end{document}
