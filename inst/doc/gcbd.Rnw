
%% use JSS class but for now with nojss option
\documentclass[nojss,shortnames,article]{jss}
\usepackage{float}

\author{Dirk Eddelbuettel\\Debian Project} % \And Second Author\\Plus Affiliation}
\title{Benchmarking single- and multi-core BLAS implementations and GPUs for use with \proglang{R}}

\Plainauthor{Dirk Eddelbuettel} % , Second Author} %% comma-separated
\Plaintitle{Benchmarking Single- and Multi-Core BLAS Implementations and GPUs for Use with R}
\Shorttitle{Benchmarking BLAS and GPUs for Use with R}

\Abstract{
  \noindent
  We provide timing results for common linear algebra subroutines across BLAS
  and GPU-based implementations. Several BLAS (Basic Linear Algebra
  Subprograms) implementations are compared. The first is the unoptimised
  reference BLAS which provides a baseline to measure against. Second is the
  Atlas tuned BLAS, configured for single-threaded mode. Third is the
  development version of Atlas, configured for multi-threaded mode. Fourth is
  the optimised and multi-threaded Goto BLAS. Fifth is the multi-threaded
  BLAS contained in the commercial Intel MKL package. We also measure the
  performance of two graphics processing units (GPU) based implementations
  for \proglang{R} \citep{RCore:R} provided by the packages \pkg{gputools}
  \citep{cran:gputools} and \pkg{magma} \citep{cran:magma}, a CPU/GPU hybrid.

  Several frequently-used linear algebra computations are compared across
  BLAS (and LAPACK) implementations and via GPU computing: matrix
  multiplication as well as QR, SVD and LU decompositions.  The tests are
  performed from an end-user perspective, and `net' times (including all
  necessary data transfers) are compared.

  While results are by their very nature dependent on the hardware of the
  test platforms, a few general lessons can be drawn. Unsurprisingly,
  accelerated BLAS clearly outperform the reference implementation.
  Similarly, multi-threaded BLAS hold a clear advantage over single-threaded
  BLAS when used on modern multi-core machines. Between the multi-threaded
  BLAS implementations, Goto is seen to have a slight advantage over MKL and
  Atlas.  GPU computing is showing promise but requires relatively large
  matrices to outperform multi-threaded BLAS.  Lastly, the hybrid approach
  that is being implemented by the Magma project is intriguing but exhibits
  problems with stability in its current version.
}

\Keywords{BLAS, Atlas, Goto, MKL, GPU, \proglang{R}, Linux} %% at least one keyword must be supplied
\Plainkeywords{BLAS, Atlas, Goto, MKL, GPU, R, Linux} %% without formatting

%% publication information
%% NOTE: Typically, this can be left commented and will be filled out by the technical editor
%% \Volume{13}
%% \Issue{9}
%% \Month{September}
%% \Year{2004}
%% \Submitdate{2004-09-29}
%% \Acceptdate{2004-09-29}

\Address{
  Dirk Eddelbuettel \\
  Debian Project \\
  River Forest, IL, USA\\
  %% Telephone: +43/1/31336-5053
  %% Fax: +43/1/31336-734
  E-mail: \email{edd@debian.org}\\
  URL: \url{http://dirk.eddelbuettel.com}
}

%% need no \usepackage{Sweave.sty}

% ------------------------------------------------------------------------

\begin{document}
\SweaveOpts{engine=R,eps=FALSE,echo=FALSE,prefix.string=figures/chart}
%\VignetteIndexEntry{BLAS and GPU Benchmarking}
%\VignetteDepends{gputools,magma
%\VignetteKeywords{blas,gpu,atlas,mkl,goto}
%\VignettePackage{gcbd}

\shortcites{Brodtkorb_et_al_2010,Blackford_et_al:2002,lapack}

%% Note that you should use the \pkg{}, \proglang{} and \code{} commands.

<<prelim,echo=FALSE,print=FALSE>>=
options(width=50)
library(gcbd)
gcbd.version <- packageDescription("gcbd")$Version
gcbd.date <- packageDescription("gcbd")$Date
now.date <- strftime(Sys.Date(), "%B %d, %Y")
# dput(brewer.pal(7, "Set1"))
cols <- c("#E41A1C", "#377EB8", "#4DAF4A", "#984EA3", "#FF7F00", "#FFF33", "#A65628")[-6]
# dput(brewer.pal(8,"Paired"))
paircols <- c("#A6CEE3", "#1F78B4", "#B2DF8A", "#33A02C", "#FB9A99", "#E31A1C", "#FDBF6F", "#FF7F00")
# create figures/ if not present
if ( ! (file.exists("figures") && file.info("figures")$isdir) ) dir.create("figures")
@
%

Detailed comments and suggestions from Roger Bivand, Allan Engelhardt, Bryan
Lewis, Junji Nakano, Mark Seligman, Brian Smith and Luke Tierney are very
gratefully acknowledged. All remaining errors are mine.

This version corresponds to \pkg{gcbd} \Sexpr{gcbd.version} and was compiled
on \Sexpr{now.date}.

\textbf{This is a draft version with possibly premature results. Please do
  not circulate.}


%\pagebreak
\section[Introduction]{Introduction}

Analysts are often eager to reap the maximum performance from their computing
platforms.  A popular suggestion in recent years has been to consider
optimised basic linear algebra subprograms (BLAS).  Optimised BLAS libraries
have been included with some (commercial) analysis platforms for a decade
\citep{Moler:2000}, and have also been available for (at least some) Linux
distributions for an equally long time \citep{Maguire:1999}.  Setting BLAS up
can be daunting: the \proglang{R} language and environment devotes a detailed
discussion to the topic in its \textsl{Installation and Administration}
manual \citep[appendix A.3.1]{RCore:InstAdmin}.

Among the available BLAS implementations, several popular choices have
emerged. Atlas (an acronym for \textsl{Automatically Tuned Linear Algebra
  System}) is popular as it has shown very good performance due to its
automated and cpu-specific tuning \citep{Whaley_Dongarra:1999,
  Whaley_Petitet:2005}. It is also licensed in such a way that it permits
redistribution leading to fairly wide availability of Atlas.\footnote{The
  Atlas FAQ lists Maple, Matlab and Mathematica as using Atlas, and
  mentions that GSL, Octave, R, Scientific Python, and Scilab can be used with
  Atlas.}  We deploy Atlas in both a single-threaded and a multi-threaded
configuration. Another popular BLAS implementation is Goto BLAS which is named
after its main developer, Kazushige Goto \citep{Goto_VanDeGeijin:2008}. While
`free to use', its license does not permit redistribution putting the onus of
configuration, compilation and installation on the end-user.  Lastly, the
Intel Math Kernel Library (MKL), a commercial product, also includes an
optimised BLAS library.

A recent addition to the toolchain of high-performance computing are
graphical processing units (GPUs).  Originally designed for optimised
single-precision arithmetic to accelerate computing as performed by graphics
cards, these devices are increasingly used in numerical analysis.  Earlier
criticism of insufficient floating-point precisions or severe performance
penalties for double-precision calculation are being addressed by the newest
models. Dependence on particular vendors remains a concern with NVidia's CUDA
toolkit \citep{nVidia:2010} currently still the preferred development choice
whereas the newer OpenCL standard \citep{OpenCL:2010} may become a more
generic alternative that is independent of hardware vendors.
\citet{Brodtkorb_et_al_2010} provide an excellent recent survey.

But what has been lacking is a comparison of the effective performance of these
alternatives.  This paper works towards answering this question.  By analysing
performance across five different BLAS implementations---as well as two
GPU-based solutions---we are able to provide a reasonably broad comparison.

Performance is measured as an end-user would experience it: we record
computing times from launching commands in the interactive \proglang{R}
environment \citep{RCore:R} to their completion.  While implemented in
\proglang{R}, these benchmark results are more general and valid beyond the
\proglang{R} system as there is only a very thin translation layer between
the higher-level commands and the underlying implementations (such as, say,
\code{dgemm} for double-precision matrix multiplications) in the respective
libraries.  This lack of (strong) dependence on the test environment makes
our results more generally applicable. However, \proglang{R} is very useful
as an environment to generate test data, execute the benchmarks, and to
collect the results which are subsequently analysed and visualized.

The rest of the paper is organised as follows. In the next section, the
technical background is briefly discussed. The implementation of our
benchmark tests is outlined in section 3. We provide results in section 4,
before a summary concludes in section 5.

\section{Background}

Basic Linear Algrebra Subprograms (BLAS) provide an Application Programming
Interface (API) for linear algebra.  For a given task such as, say, a
multiplication of two conformant matrices, an interface is described via a
function declaration, in this case \code{sgemm} for single precision and
\code{dgemm} for double precision. The actual implementation becomes
interchangeable thanks to the API definition and can be supplied by different
approaches or algorithms.  This is one of the fundamental code design
features we are using here to benchmark the difference in performance from
different implementations.

A second key aspect is the difference between static and shared linking.  In
static linking, object code is taken from the underlying library and copied
into the resulting executable.  This has several key implications. First, the
executable becomes larger due to the copy of the binary code. Second, it
makes it marginally faster as the library code is present and no additional
lookup and subsequent redirection has to be performed. The actual amount of
this performance penalty is the subject of near-endless debate. We should
also note that this usually amounts to only a small load-time penalty
combined with a function pointer redirection---the actual compuation effort
is unchanged as the actual object code is identical. Third, it makes the program
more robust as fewer external dependencies are required.  However, this last
point also has a downside: no changes in the underlying library will be
reflected in the binary unless a new build is executed.  Shared library
builds, on the other hand, result in smaller binaries that may run marginally
slower---but which can make use of different libraries without a rebuild.

That last feature is key here: it offers us the ability to use different BLAS
implementations in a `plug and play' mode which facilitates comparisons.
So because of both the standardised interface of the BLAS, and the fact that we
have several alternative implementations at our disposal, we can switch
between these alternatives. To do so easily makes use of a package mechanism
used by Ubuntu, the Debian-based Linux distribution we employ. However, a
simpler (yet less robust) approach would also be available. This technical
aspect is discussed further below.

The first available BLAS implementation stems from the original unoptimised
code on Netlib. On Debian-based systems, it is provided by the (source)
package \pkg{blas} \citep{Blackford_et_al:2002} and used with the
\pkg{lapack} package \citep{lapack}. This implementation is commonly referred
to as `reference BLAS' (and we will use `ref' as a shorthand) as it provides
a reference implementation.

The second and third BLAS implementations are provided by Atlas
\citep{Whaley_Dongarra:1999,Whaley_Petitet:2005} which stands for
\textsl{Automatically Tuned Linear Algebra Software} as it optimises its
performance and parameters (such as cache sizes) during its initial build.
Another notable aspect is its liberal licensing which permits wide
distribution. Consequently, Atlas is available on most if not all Linux
distributions. Windows binaries are also widely available.  We used two
different Atlas versions. The first one is the (pre-compiled) version in the
Ubuntu and Debian releases. It is based on Atlas 3.6.0 and built only for
single-threaded operation.  The second version is based on the current Atlas
development version 3.9.25, built for multi-threaded mode and locally
compiled.\footnote{A critical change enabling multi-threaded mode which we
  added was also filed as Debian bug report \#595326 and will be reflected in
  future Debian / Ubuntu packages.} We will refer to the second variant as
`Atlas39' and `Atl39' when we report results below.

The fourth BLAS implementation is provided by the Goto BLAS. Upon the required
user registration, these are freely available from the University of Texas,
albeit under a license that prohibits redistribution.  This prevents
inclusion into popular Linux distributions, a clear disadvantage for easy
installation and de-installation.  However, the contributed Debian repository
at the Institute of Statistical Mathematics in Japan provides a `helper
package' \citep{nakano_nakama:2009} which, given the required registration
information, downloads the source code from the University of Texas site, and
then configures and compiles the Goto BLAS in such a manner that a local
binary Debian package is produced---which is also optimised for the local
installation.  This permits us to use the Goto BLAS via the resulting package
as a fourth BLAS alternative.

The fifth available BLAS implementation is part of the Intel Math Kernel
Library (MKL), a commercial product. However, Ubuntu release 9.10
(``karmic'') contains a set of packages sponsored by Revolution Analytics
which comprises version 10.2 (dated March 2009) of the Intel MKL in a setup
directly useable by \proglang{R}. We use these packages here as a fifth set
of optimised BLAS.

The first \proglang{R} extension for graphics-processing units has been implemented
by the \pkg{gputools} package \citep*{cran:gputools}. It provides a number of
functions which use the GPU instead of the CPU which results in a significant
performance increase for `large enough' problems.  Because data has to be transferred
from the CPU to the GPU, a fixed cost in communications has to be borne by
every invocation of the GPU.  For sizeable problems, this cost can be
outweighed by the benefits of the massively parallel GPU computation.
Exactly where the indifference point lies beyond which GPU computing has an
advantage is unfortunately dependent on the particular problem and algorithm
as well as the given hardware and software combination.

A recent addition to the set of \proglang{R} packages is \pkg{magma}
\citep{cran:magma} which interfaces the \pkg{Magma} library project of the
same name \citep{Tomov_et_al:2009,Tomov_et_al:2010} (where we will capitalize
the name of the library to distinguish it from the \proglang{R} package). The
stated goal of the Magma project it to \textsl{develop a dense linear algebra
  library similar to LAPACK but for heterogeneous/hybrid architectures,
  starting with current "Multicore+GPU" systems}.  The current release of
Magma is version 0.2 which is available for Linux systems with NVidia's CUDA
toolkit. It provides LU, QR, and Cholesky factorizations as well as linear
solvers based on these along with implementations of several BLAS function
including \code{dgemm}, \code{dgemv}, and \code{dsymv} (and well as their
single-precision counterparts).  Figure~\ref{fig:magma} illustrates the changes of
relative workload between CPU and GPU when using Magma for a
(single-precision) QR decompostion.  It clearly depicts how the share of the
total computing effort that goes to the GPU increases as a function of the
matrix size.

\begin{figure}[htb]
  \centering
  \includegraphics[width=4in]{MagmaTimes.png}
  \caption{Breakdown of CPU and GPU computing percentages for
    single-precision QR decomposition using the hybrid Magma
    approach. (Source: \citet[p.7]{Tomov_et_al:2009})}
  \label{fig:magma}
\end{figure}

A flexible and hybrid approach has the potential to improve upon solutions
that use either the CPU or the GPU. To the best of our knowledge, this paper
offers the first benchmark comparison of Magma versus BLAS implementations.

Finally, an important, and possibly under-appreciated, topic is how to
balance explicitly parallel code that distributes load across cores with
multi-threaded BLAS.  Execution of Code may already be parallelised in a
`coarse-grained' fashion across all local cores or cpus, maybe via tools that
explicitly spawn multiple threads, maybe via compiler-driven directives as
for example with Open MP, or maybe via cluster-computing tools across a local
network.  If such a setup `booked' all available cores, a key assumption of
multi-threaded BLAS no longer holds: other cores are not idle but as busy. In
this case contention arises between the explicit parallelism and the implicit
parallelism from the multi-threaded BLAS, and performance is bound to
suffer. \citet{bivand2010} discusses this issues and provides several
illustrations using examples from data-intensive GIS applications.  The
simplest remedy is to withdraw one of the mechanisms for multi-threaded use by
limiting the number of cores to one.

\section{Implementation}

\subsection{Requirements}

In order to undertake the automated benchmarking, we need to be able to
switch between different implementations of the BLAS API.  As discussed
above, dynamic libraries are one possible solution that avoids having to
rebuild \proglang{R} explicitly for each library.  However, this also
requires that \proglang{R} itself is built with shared-library support, as
well as with support for external BLAS and LAPACK libraries.  This
requirement is however the default configuration on Debian and Ubuntu systems.

The reference BLAS as well as Atlas have been available for essentially all
Debian and Ubuntu releases. Atlas 3.9.25 was packaged locally; the package and
helper scripts are available upon request. The Intel MKL is available for
Ubuntu following the Revolution R upload for Ubuntu release 9.10.

For Goto BLAS, we are using a helper script provided in the contributed
\texttt{gotoblas2-helper} package \citep{nakano_nakama:2009,Nakama:2010}.
This package arranges for a download of the Goto BLAS sources (provided the
required account and password information for the University of Texas
software download center) and an automated Debian package build and
installation via the command \code{sudo /etc/init.d/gotoblas2-helper
  start}. Note that the initial invocation of this command will trigger a
build of the package which may take up to two hours.  While designed for
Debian, it also works perfectly on the Ubuntu systems used here.\footnote{The
  \texttt{/etc/init.d/gotoblas2-helper} script required one change from
  \texttt{/bin/sh} to \texttt{/bin/bash}.}

For GPU-based testing we require the \proglang{R} packages \pkg{gputools} and
\pkg{magma} which in turn require support for CUDA and the NVidia SDK (as
well as appropriate NVidia hardware). Detailed installation instructions are
provided by either package so we will defer to these and assume that the
packages are in fact installed.  Helper scripts in our package will then
verify this availability while the benchmark is executed.

\subsection{Benchmark Implementation}

The benchmarks described in this paper are produced by the package \pkg{gcbd}
which is part of the larger \pkg{gcb} project \citep*{rforge:gcb} on the
R-Forge hosting site \citep{RJournal:Theussl+Zeileis:2009}. The \pkg{gcbd}
package (where the acronym expands to `GPU/CPU Benchmarking on Deb-based
systems') contains a number of \proglang{R} helper functions as well as an
actual benchmarking script which is executed.

The helper functions fall into two groups: utilities, and benchmarks. The
utilities fall into several categories:
\begin{itemize}
\item initial feature detection via a function \code{requirements()} which asserts
  that a number of testable features of the host operating system are met;
\item feature tests via functions \code{hasMagma()} and \code{hasGputools()}
  allowing the benchmark script to bypass GPU-based tests in systems without
  a GPU;
\item installation and removal functions which interface the package
  management layer and install (or remove) the Atlas, MKL or Goto BLAS
  packages, respectively, which helps ensure that at any one point in time
  only one accelerated BLAS library package is present;
\item database creation where the results database (and table schema) is
  created if not already present;
\item recording of simulation results in the database.
\end{itemize}

The benchmark functions can also be categorized:
\begin{itemize}
\item creation of random data for standard \pkg{Matrix} or \pkg{magma}
  objects, respectively;
\item actual benchmarking code for
  \begin{itemize}
  \item matrix crossproducts;
  \item SV decomposition;
  \item QR decomposition;
  \item LU decomposition;
  \end{itemize}
  for BLAS, \pkg{gputools} and \pkg{magma}, respectively.
\end{itemize}

For the QR decomposition, we set the flag \code{LAPACK=TRUE}. For
\proglang{R}, the default QR operation is provided by LINPACK which uses
level 1 BLAS operations where LAPACK can reap a larger benefit from
accelerated or optimised BLAS libraries.  For the LU decompositions, we use
the function from the \pkg{Matrix} package by \citet{cran:matrix}.

\subsection{Benchmark script}

The benchmark execution can then be triggered by the script
\code{benchmark.r} in the subdirectory \code{scripts} of the \pkg{gcbd}
package. It is implemented an executable \proglang{R} script which uses the
\pkg{getopt} package \citep{cran:getopt} for command-line parsing:

\begin{verbatim}
$ ./benchmark.r -h
Usage: benchmark.r [-[-verbose|v]] [-[-help|h]] [-[-nobs|n] <integer>]
                   [-[-runs|r] <integer>] [-[-benchmark|b] <character>]
    -v|--verbose      verbose operations, default is false
    -h|--help         help on options
    -n|--nobs         number of rows and columns in matrix, default is 250
    -r|--runs         number of benchmark runs, default is 30
    -b|--benchmark    benchmark to run (matmult, qr, svd, lu), default is matmult
\end{verbatim}

Each benchmark experiment consists of $r$ runs for a matrix of size $n \times
n$ observations using the chosen benchmark---matrix crossproduct or one of
the QR, LU or SVD decompositions---over all five BLAS implementations and two
GPU packages.  The run against the GPU packages is optional and dependent on
the GPU packages being present.

At the end of each benchmark experiment, the results are appended to a SQLite
database.

We use the `elapsed time' as measured by the \proglang{R} function
\code{system.time()}.  This measure is preferable over the sum of system time
and user time which adds up total cputime---but without adjusting for multiple
threads or cores. Elapsed time correctly measures from start to finish,
whether one or multiple threads or cores are involved (but is susceptible to
be influenced by system load).

Using this script \code{benchmark.r}, users can collect benchmark results on
their systems.  These could be used to aggregate more performance data which
could then be used to estimate realistic performance numbers for a much wider
variety of hardware configurations.  Doing so is beyond the scope of this
paper but a possible venue for future work.

\subsection{Alternative implementation}

On platforms that do not have access to pre-built BLAS library packages, an
alternative approach could consist of locally installing the different
libraries into subdirectories of, say, \code{/opt/blas}. One could then use
the environment variable \verb|LD_LIBRARY_PATH| to select one of these
directories at a time.  However, such an approach places a larger burden on
the user of the benchmarking software as she would have to download,
configure, compile and install the BLAS
libraries which is typically not a trivial step.

\subsection{Hardware and software considerations}
\label{subsec:hardware}

For benchmarking linear algebra performance, hardware and software aspects
matter greatly for the overall results. Different CPUs, different GPUs,
different memory type as well as different compilers (or operating systems)
may generate different performance profiles.

We have been running the results presented here on these two platforms:
\begin{itemize}
\item a four-core Intel i7 920, 2.67 GHz clockspeed, 8192kb cpu cache, 6 gb ram,
  in hyperthreaded mode for eight visible cores, running Ubuntu 10.4 in 64-bit mode;
\item a dual-cpu four-core Intel Xeon 5570, 2.96 Ghz clockspeed, 8192kb cpu
  cache, 16gb ram, in hyperthreaded mode
  for sixteen visible cores, running Ubuntu 10.4 in 64-bit mode.
\end{itemize}

The i7 system also has a NVidia Quadro FX4800 GPU with 192 cores and 1.5 gb
of memory. %This GPU card is
%the only one we have tested the benchmarks against.

Different hardware platforms could be reflected by other users installing the
\pkg{gcbd} package on their system and reporting results back by supplying
the resulting SQLite database files.

The software configuration was held constant by runnning on 64-bit Ubuntu
10.4 in both cases.

It should be noted that hyper-threading is potential detraction. While it was
not possible to reconfigure the machines used here, it could be informative
to compare results on identical hardware with hyper-threading turned on and
off, respectively.

Lastly, comparing across operating system would also be of interest. While
the test platform used here makes extensive use of the packaging system
available on Debian / Ubuntu, it would of course be possible to set up
something similar on, say, OS X to measure the performance of the vecLib system.

%\pagebreak
\section[Results]{Results}

<<data,print=FALSE>>=
dbcon <- dbConnect(dbDriver("SQLite"), dbname=system.file("sql", "gcbd.sqlite", package="gcbd"))
i7 <- dbGetQuery(dbcon, 'select * from benchmark where host="max" order by nobs')
xeon <- dbGetQuery(dbcon, 'select * from benchmark where host="xeon_X5570" order by nobs')
invisible(dbDisconnect(dbcon))
D <- subset(i7[,-c(1:2,5)], type=="matmult")
@

\subsection{BLAS Comparison}

We present the benchmark results (which are also included in the \pkg{gcbd}
package in the file \code{sql/gcbd.sqlite}).  We will show one type of
benchmark per test architecture (currently one of i7 or xeon as detailed in
section~\ref{subsec:hardware}) with times first shown on a regular scale
(with the matrix dimension on the horizontal axis and the elapsed time in
seconds on the vertical axis), and also in a log/log plot with both matrix
dimension and elapsed time on a logarithmic scale in order to better
differentiate between alternatives.\footnote{I am particularly grateful to
  Allan Engelhardt for suggesting to switch from a plot with logged y-axis to
  a log-log plot.}

\subsubsection{Matrix Multiplication: i7 with GPU}

\setkeys{Gin}{width=0.99\textwidth}
\begin{figure}[H]
  \centering
<<matmulti7,fig=TRUE,height=5.25,width=11>>=
D <- subset(i7[,-c(1:2,5)], type=='matmult')
op <- par(mfrow=c(1,2))
matplot(x=D[,"nobs"], y=D[,c("ref","atlas","atl39","mkl","gotob","gpu")], type='l', lty=1, lwd=3, col=cols,
        xlab="Matrix dimension", ylab="Time in seconds", main="Matrix Multiplication")
legend("topleft", legend=c("Ref","Atlas","Atl39","MKL","Goto","GPU"), bty="n", col=cols, lty=1, lwd=3)
matplot(x=D[,"nobs"], y=D[,c("ref","atlas","atl39","mkl","gotob","gpu")], type='l', lty=1, lwd=3, col=cols,
        xlab="Matrix dimension", ylab="Time in seconds", main="Matrix Multiplication", log="xy")
legend("bottomright", legend=c("Ref","Atlas","Atl39","MKL","Goto","GPU"), bty="n", col=cols, lty=1, lwd=3)
par(op)
@
\caption{Matrix multiplications on i7 with GPU}
\end{figure}

It is immediately apparent that the reference BLAS underperform very
significantly. Similarly, the single-threaded Atlas performance is also
dominated by the multi-threaded BLAS (Atlas39, Goto, MKL) and the GPU-based
gputools.

We also see (especially in the log/log plot) that the GPU-based solution
bears a clear penalty for smaller dimensions. It only crosses below the
single-threaded Atlas around $N=1000$ and approaches the best performing
multi-threaded BLAS at the very end for $N=5000$.  On the other hand, it
clearly exhibits a much slower slope implying a lesser time penalty as matrix
sizes increases.

The three multi-threaded BLAS implementations are essentially
indistinguishable for this test and hardware platform.

\subsubsection{Matrix Multiplication: Xeon}

\begin{figure}[H]
  \centering
<<matmultxeon,fig=TRUE,height=5.25,width=11>>=
D <- subset(xeon[,-c(1:2,5)], type=='matmult')
op <- par(mfrow=c(1,2))
matplot(x=D[,"nobs"], y=D[,c("ref","atlas","atl39","mkl","gotob")], type='l', lty=1, lwd=3, col=cols,
        xlab="Matrix dimension", ylab="Time in seconds", main="Matrix Multiplication")
legend("topleft", legend=c("Ref","Atlas","Atl39","MKL","Goto"), bty="n", col=cols, lty=1, lwd=3)
matplot(x=D[,"nobs"], y=D[,c("ref","atlas","atl39","mkl","gotob")], type='l', lty=1, lwd=3, col=cols,
        xlab="Matrix dimension", ylab="Time in seconds", main="Matrix Multiplication", log="xy")
legend("bottomright", legend=c("Ref","Atlas","Atl39","MKL","Goto"), bty="n", col=cols, lty=1, lwd=3)
par(op)
@
\caption{Matrix multiplications on xeon}
\end{figure}

As above, the reference BLAS underperform significantly, and single-threaded
Atlas is dominated by all multi-threaded BLAS implementation. On the other
hand, on this platform the log/log plot offers a better differentiation
between the multithreaded BLAS implementations. Goto is clearly ahead of its
competitors. MKL shows a lower slope and higher times for small sizes hinting
at some suboptimal configuration.  Atlas39 beats MKL for essentially all
sizes but cannot catch Goto BLAS' performances.


\subsubsection{QR Decomposition: i7 with GPU}

\begin{figure}[H]
  \centering
<<qri7,fig=TRUE,height=5.25,width=11>>=
D <- subset(i7[,-c(1:2,5)], type=='qr')
op <- par(mfrow=c(1,2))
matplot(x=D[,"nobs"], y=D[,c("ref","atlas","atl39","mkl","gotob","gpu")], type='l', lty=1, lwd=3, col=cols,
        xlab="Matrix dimension", ylab="Time in seconds", main="QR Decomposition")
legend("topleft", legend=c("Ref","Atlas","Atl39","MKL","Goto","GPU"), bty="n", col=cols, lty=1, lwd=3)
matplot(x=D[,"nobs"], y=D[,c("ref","atlas","atl39","mkl","gotob","gpu")], type='l', lty=1, lwd=3, col=cols, log="xy",
        xlab="Matrix dimension", ylab="Time in seconds", main="QR Decomposition")
legend("bottomright", legend=c("Ref","Atlas","Atl39","MKL","Goto","GPU"), bty="n", col=cols, lty=1, lwd=3)
par(op)
@
\caption{QR decomposition on i7 with GPU}
\end{figure}

As before, reference BLAS are behind all other implementations. The
single-threaded Atlas is on-par with the multi-threaded Atlas39---and the GPU
solution. Goto and MKL beat all others, with Goto dominating overall.  The
log/log plot once again illustrates the higher `startup cost' of the GPU
solution due to the communications cost being high (in relatively terms) for
small matrices where the computations are still very fast.

\subsubsection{QR Decomposition: Xeon}

\begin{figure}[H]
  \centering
<<qrxeon,fig=TRUE,height=5.25,width=11>>=
D <- subset(xeon[,-c(1:2,5)], type=='qr')
op <- par(mfrow=c(1,2))
matplot(x=D[,"nobs"], y=D[,c("ref","atlas","atl39","mkl","gotob")], type='l', lty=1, lwd=3, col=cols,
        xlab="Matrix dimension", ylab="Time in seconds", main="QR Decomposition")
legend("topleft", legend=c("Ref","Atlas","Atl39","MKL","Goto"), bty="n", col=cols, lty=1, lwd=3)
matplot(x=D[,"nobs"], y=D[,c("ref","atlas","atl39","mkl","gotob")], type='l', lty=1, lwd=3, col=cols,
        xlab="Matrix dimension", ylab="Time in seconds", main="QR Decomposition", log="xy")
legend("bottomright", legend=c("Ref","Atlas","Atl39","MKL","Goto"), bty="n", col=cols, lty=1, lwd=3)
par(op)
@
\caption{QR decomposition on xeon}
\end{figure}

Results fall basically into three groups. The Reference BLAS are behind
everybody. A second group of both Atlas variants and MKL forms the middle
with MKL having a slight edge over the Atlas libraries. However, Goto is
ahead of everybody (yet shows a somewhat surprising non-linearity in the
log/log plot).

\subsubsection{SVD Decomposition: i7 with GPU}

\begin{figure}[H]
  \centering
<<svdi7,fig=TRUE,height=5.25,width=11>>=
D <- subset(i7[,-c(1:2,5)], type=='svd')
op <- par(mfrow=c(1,2))
matplot(x=D[,"nobs"], y=D[,c("ref","atlas","atl39","mkl","gotob","gpu")], type='l', lty=1, lwd=3, col=cols,
        xlab="Matrix dimension", ylab="Time in seconds", main="SVD Decomposition")
legend("topleft", legend=c("Ref","Atlas","Atl39","MKL","Goto","GPU"), bty="n", col=cols, lty=1, lwd=3)
matplot(x=D[,"nobs"], y=D[,c("ref","atlas","atl39","mkl","gotob","gpu")], type='l', lty=1, lwd=3, col=cols, log="xy",
        xlab="Matrix dimension", ylab="Time in seconds", main="SVD Decomposition")
legend("bottomright", legend=c("Ref","Atlas","Atl39","MKL","Goto","GPU"), bty="n", col=cols, lty=1, lwd=3)
par(op)
@
\caption{SVD on i7 with GPU}
\end{figure}

For the SVD, we have the familiar ordering of Reference BLAS behind
single-threaded Atlas which is behind multi-threaded Atlas.  MKL, Goto and
GPU are all competitive, with the GPU lagging for small sizes but beating all
competitors for the largest matrix size tests.  In the log/log chart, the GPU
performance also demonstrates a much slower slope, yet along with the much
higher intercept.

\subsubsection{SVD: Xeon}

\begin{figure}[H]
  \centering
<<svdxeon,fig=TRUE,height=5.25,width=11>>=
D <- subset(xeon[,-c(1:2,5)], type=='svd')
op <- par(mfrow=c(1,2))
matplot(x=D[,"nobs"], y=D[,c("ref","atlas","atl39","mkl","gotob")], type='l', lty=1, lwd=3, col=cols,
        xlab="Matrix dimension", ylab="Time in seconds", main="SVD Decomposition")
legend("topleft", legend=c("Ref","Atlas","Atl39","MKL","Goto"), bty="n", col=cols, lty=1, lwd=3)
matplot(x=D[,"nobs"], y=D[,c("ref","atlas","atl39","mkl","gotob")], type='l', lty=1, lwd=3, col=cols,
        xlab="Matrix dimension", ylab="Time in seconds", main="SVD Decomposition", log="xy")
legend("bottomright", legend=c("Ref","Atlas","Atl39","MKL","Goto"), bty="n", col=cols, lty=1, lwd=3)
par(op)
@
\caption{SVD on xeon}
\end{figure}

On the Xeon, results are similar to the i7 with the main difference that the
multi-threaded Atlas39 is closer to its competitors, in particular the
MKL. Goto BLAS are once again ahead.


\subsubsection{LU Decomposition: i7 with GPU}

\begin{figure}[H]
  \centering
<<lui7,fig=TRUE,height=5.25,width=11>>=
D <- subset(i7[,-c(1:2,5)], type=='lu')
op <- par(mfrow=c(1,2))
matplot(x=D[,"nobs"], y=D[,c("ref","atlas","atl39","mkl","gotob")], type='l', lty=1, lwd=3, col=cols,
        xlab="Matrix dimension", ylab="Time in seconds", main="LU Decomposition")
legend("topleft", legend=c("Ref","Atlas","Atl39","MKL","Goto"), bty="n", col=cols, lty=1, lwd=3)
matplot(x=D[,"nobs"], y=D[,c("ref","atlas","atl39","mkl","gotob")], type='l', lty=1, lwd=3, col=cols, log="xy",
        xlab="Matrix dimension", ylab="Time in seconds", main="LU Decomposition")
legend("bottomright", legend=c("Ref","Atlas","Atl39","MKL","Goto"), bty="n", col=cols, lty=1, lwd=3)
par(op)
@
\caption{LU decomposition on i7 with GPU}
\end{figure}

LU decomposition results, using the function from the \pkg{Matrix} package
\citep{cran:matrix}, are similar to earlier results. Reference BLAS are by
far the slowest, single-threaded Atlas beats them clearly and loses equally
clearly to the multi-threaded BLAS. Multi-threaded Atlas is very close to
Goto and MKL, which are ever so slightly ahead and essentially
indistinguishable.  For this algorithm, no GPU-based performance numbers are
available as the current version of the \pkg{gputools} package does not
provide a LU decomposition.


\subsubsection{LU Decomposition: Xeon}

\begin{figure}[H]
  \centering
<<luxeon,fig=TRUE,height=5.25,width=11>>=
D <- subset(xeon[,-c(1:2,5)], type=='lu')
op <- par(mfrow=c(1,2))
matplot(x=D[,"nobs"], y=D[,c("ref","atlas","atl39","mkl","gotob")], type='l', lty=1, lwd=3, col=cols,
        xlab="Matrix dimension", ylab="Time in seconds", main="LU Decomposition")
legend("topleft", legend=c("Ref","Atlas","Atl39","MKL","Goto"), bty="n", col=cols, lty=1, lwd=3)
matplot(x=D[,"nobs"], y=D[,c("ref","atlas","atl39","mkl","gotob")], type='l', lty=1, lwd=3, col=cols,
        xlab="Matrix dimension", ylab="Time in seconds", main="LU Decomposition", log="xy")
legend("bottomright", legend=c("Ref","Atlas","Atl39","MKL","Goto"), bty="n", col=cols, lty=1, lwd=3)
par(op)
@
\caption{LU decomposition on xeon}
\end{figure}

On the xeon chip we see once again a clearer separation between the three accelerated BLAS
implementations on the one hand and the single-threaded Atlas and the
Reference BLAS on the other hand.  Goto has a clear edge over MKL, which is
also overtaken by Atlas39 for medium-to-large size matrices. MKL also
exihibits some irregularity for small-to-medium sized matrices.

\subsubsection{Comparison}

The charts shown above suggest a further analysis: a regression of the logarithm
of the elapsed times on the logarithm of the matrix dimension.  Rather than
showing the results of the individual regressions, we have regrouped the
different slope estimates in Figure~\ref{fig:slopeloglog}.  Here a higher
slope coefficient implies a higher increase in elapsed time per increase in
matrix dimension, and moreover in a non-linear fashion as we have taken logarithms.

\begin{figure}[H]
  \centering
<<loglogslopes,fig=TRUE,height=5.25,width=11>>=
LL <- loglogAnalysis()
DF <- LL[["slope"]]

DF[,"method"] <- ordered(as.character(DF[,"method"]),
                         levels=c("gpu", "goto", "mkl", "atl39", "atlas", "ref"))
DF[,"type"] <- ordered(as.character(DF[,"type"]),
                       levels=c("matmult", "qr", "svd", "lu"))

sb <- trellis.par.get("strip.background")
sb[["col"]][1] <- "lightgray"
trellis.par.set("strip.background", sb)

ss <- trellis.par.get("superpose.symbol")
ss[["col"]][1:6] <- cols
ss[["cex"]] <- rep(1.0, 7)
ss[["pch"]] <- rep(19, 7)
ss[["alpha"]] <- rep(0.75, 7)
trellis.par.set("superpose.symbol", ss)

print(dotplot(method  ~ value | type, group=host, data=DF,
              xlab="Slope of elapsed times to matrix dimension in log/log chart",
              key=simpleKey(text=c("i7","xeon"), space="bottom", column=2)))
@
\caption{Comparison of slopes in log/log analysis of BLAS performance}
\label{fig:slopeloglog}
\end{figure}

These results formalise the rank-ordering of BLAS implementation seen in the
analysis of the individual charts above.
\begin{itemize}
\item Unsurprisingly, Reference BLAS are clearly dominated by all other
  alternatives and exhibits the increase in elapsed time per increase in
  matrix dimension.
\item Single-threaded Atlas is clearly improving on the Reference
  BLAS but is dominated by the other multi-threaded libraries and
  the GPU-based solution; the QR decompostion is an exception where both Atlas
  variants and the MKL are comparable and just behind Goto.
\item Multi-threaded Atlas39 is roughly comparable to the MKL (and ahead for
  matrix multiplication) and just behind Goto BLAS.
\item The MKL Blas perform well, generally on-par with or ahead of Atlas39
  but trailing Goto BLAS for two of the four algorithms.
\item Goto BLAS are ahead for the QR and SVD tests, and on par or behind
  for matrix multiplication and LU decompositions.
\item GPU computing is seen to have the lowest slope corresponding to the lowest cost
  per additional matrix dimension increases---but this has to be balanced
  with the cost for smaller to medium sizes which can be seen from the
  corresponding analysis for intercepts (not shown but available in the
  package).
\end{itemize}

\subsection{Magma: GPU and BLAS combined}

\pkg{Magma}, as a hybrid system comprising both CPU and GPU-based computing, has a
lot of potential for improving over solutions using just one of the
processing units. In this section we are trying to measure just how much of
this potential is already realised with the early versions of Magma.

% \subsubsection{Direct usage and comparison to BLAS}

% \begin{figure}[H]
%   \centering
% < <fig=TRUE,height=5.25,width=11> >=
% D <- subset(i7[,-c(1:2,5)], type=='matmult')
% op <- par(mfrow=c(1,2))
% matplot(x=D[,"nobs"], y=D[,c("atlas","magmaAtlas","gotob","magmaGoto", "mkl", "magmaMkl")],
%         type='l', lty=1, lwd=3, col=paircols,
%         xlab="Matrix dimension", ylab="Time in seconds", main="Matrix Multiplication")
% legend("topleft", legend=c("Atlas","Atlas/Magma", "Goto", "Goto/Magma", "MKL", "MKL/Magma"),
%        bty="n", col=paircols, lty=1, lwd=3)
% matplot(x=D[,"nobs"], y=D[,c("atlas","magmaAtlas","gotob","magmaGoto", "mkl", "magmaMkl")],
%         type='l', lty=1, lwd=3, col=paircols, log="xy",
%         xlab="Matrix dimension", ylab="Time in seconds", main="Matrix Multiplication")
% legend("bottomright",
%        legend=c("Atlas","Atlas/Magma", "Goto", "Goto/Magma", "MKL", "MKL/Magma"),
%        bty="n", col=paircols, lty=1, lwd=3)
% par(op)
% @
% \end{figure}

% \begin{figure}[H]
%   \centering
% < <fig=TRUE,height=5.25,width=11 > >=
% D <- subset(i7[,-c(1:2,5)], type=='qr')
% op <- par(mfrow=c(1,2))
% matplot(x=D[,"nobs"], y=D[,c("atlas","magmaAtlas","gotob","magmaGoto", "mkl", "magmaMkl")],
%         type='l', lty=1, lwd=3, col=paircols,
%         xlab="Matrix dimension", ylab="Time in seconds", main="QR Decomposition")
% legend("topleft", legend=c("Atlas","Atlas/Magma", "Goto", "Goto/Magma", "MKL", "MKL/Magma"),
%        bty="n", col=paircols, lty=1, lwd=3)
% matplot(x=D[,"nobs"], y=D[,c("atlas","magmaAtlas","gotob","magmaGoto", "mkl", "magmaMkl")],
%         type='l', lty=1, lwd=3, col=paircols, log="xy",
%         xlab="Matrix dimension", ylab="Time in seconds", main="QR Decomposition")
% legend("bottomright",
%        legend=c("Atlas","Atlas/Magma", "Goto", "Goto/Magma", "MKL", "MKL/Magma"),
%        bty="n", col=paircols, lty=1, lwd=3)
% par(op)
% @
% \end{figure}

% \begin{figure}[H]
%   \centering
% < < fig=TRUE,height=5.25,width=11 > >=
% D <- subset(i7[,-c(1:2,5)], type=='svd')
% op <- par(mfrow=c(1,2))
% matplot(x=D[,"nobs"], y=D[,c("atlas","magmaAtlas","gotob","magmaGoto", "mkl", "magmaMkl")],
%         type='l', lty=1, lwd=3, col=paircols,
%         xlab="Matrix dimension", ylab="Time in seconds", main="SVD Decomposition")
% legend("topleft", legend=c("Atlas","Atlas/Magma", "Goto", "Goto/Magma", "MKL", "MKL/Magma"),
%        bty="n", col=paircols, lty=1, lwd=3)
% matplot(x=D[,"nobs"], y=D[,c("atlas","magmaAtlas","gotob","magmaGoto", "mkl", "magmaMkl")],
%         type='l', lty=1, lwd=3, col=paircols, log="xy",
%         xlab="Matrix dimension", ylab="Time in seconds", main="SVD Decomposition")
% legend("bottomright",
%        legend=c("Atlas","Atlas/Magma", "Goto", "Goto/Magma", "MKL", "MKL/Magma"),
%        bty="n", col=paircols, lty=1, lwd=3)
% par(op)
% @
% \end{figure}

% \subsubsection{Performance relative to reference BLAS}

% \begin{figure}[H]
%   \centering
% < < fig=TRUE,height=5.25,width=11 > >=
% op <- par(mfrow=c(1,2))
% D <- subset(i7[,-c(1:2,5)], type=='matmult')
% N <- cbind(D[,"nobs"],
%            D[,"ref"]/D[,"atlas"],
%            D[,"ref"]/D[,"magmaAtlas"],
%            D[,"ref"]/D[,"gotob"],
%            D[,"ref"]/D[,"magmaGoto"],
%            D[,"ref"]/D[,"mkl"],
%            D[,"ref"]/D[,"magmaMkl"],
%            D[,"ref"]/D[,"gpu"])
% matplot(x=D[,"nobs"], y=N[,-1],
%         type='l', lty=1, lwd=3, col=paircols, #pch=".",
%         xlab="Matrix dimension", ylab="Performance relative to Ref.BLAS",
%         main="Matrix Multiplication:\nRatio of Reference BLAS to given BLAS")
% legend("bottomright",
%        legend=c("Atlas","Atlas/Magma", "Goto", "Goto/Magma", "MKL", "MKL/Magma", "GPU"),
%        bty="n", col=paircols, lty=1, lwd=3)

% D <- subset(i7[,-c(1:2,5)], type=='qr')
% N <- cbind(D[,"nobs"],
%            D[,"ref"]/D[,"atlas"],
%            D[,"ref"]/D[,"magmaAtlas"],
%            D[,"ref"]/D[,"gotob"],
%            D[,"ref"]/D[,"magmaGoto"],
%            D[,"ref"]/D[,"mkl"],
%            D[,"ref"]/D[,"magmaMkl"],
%            D[,"ref"]/D[,"gpu"])
% matplot(x=D[,"nobs"], y=N[,-1],
%         type='l', lty=1, lwd=3, col=paircols, #pch=".",
%         xlab="Matrix dimension", ylab="Performance relative to Ref.BLAS",
%         main="QR Decomposition:\nRatio of Reference BLAS to given BLAS")
% legend("bottomright",
%        legend=c("Atlas","Atlas/Magma", "Goto", "Goto/Magma", "MKL", "MKL/Magma", "GPU"),
%        bty="n", col=paircols, lty=1, lwd=3)
% @
% \end{figure}


% \begin{figure}[H]
%   \centering
% < <fig=TRUE,height=5.25,width=11 > >=
% op <- par(mfrow=c(1,2))
% D <- subset(i7[,-c(1:2,5)], type=='svd')
% N <- cbind(D[,"nobs"],
%            D[,"ref"]/D[,"atlas"],
%            D[,"ref"]/D[,"magmaAtlas"],
%            D[,"ref"]/D[,"gotob"],
%            D[,"ref"]/D[,"magmaGoto"],
%            D[,"ref"]/D[,"mkl"],
%            D[,"ref"]/D[,"magmaMkl"],
%            D[,"ref"]/D[,"gpu"])
% matplot(x=D[,"nobs"], y=N[,-1],
%         type='l', lty=1, lwd=3, col=paircols, #pch=".",
%         xlab="Matrix dimension", ylab="Performance relative to Ref.BLAS",
%         main="SVD Decomposition:\nRatio of Reference BLAS to given BLAS")
% legend("bottomright",
%        legend=c("Atlas","Atlas/Magma", "Goto", "Goto/Magma", "MKL", "MKL/Magma", "GPU"),
%        bty="n", col=paircols, lty=1, lwd=3)

% D <- subset(i7[,-c(1:2,5)], type=='lu' & nobs<=2000)
% N <- cbind(D[,"nobs"],
%            D[,"ref"]/D[,"atlas"],
%            D[,"ref"]/D[,"magmaAtlas"],
%            D[,"ref"]/D[,"gotob"],
%            D[,"ref"]/D[,"magmaGoto"],
%            D[,"ref"]/D[,"mkl"],
%            D[,"ref"]/D[,"magmaMkl"])
% matplot(x=D[,"nobs"], y=N[,-1],
%         type='l', lty=1, lwd=3, col=paircols, #pch=".",
%         xlim=c(min(D[,"nobs"]), 4000),  # fill in for missing
%         xlab="Matrix dimension", ylab="Performance relative to Ref.BLAS",
%         main="LU Decomposition:\nRatio of Reference BLAS to given BLAS")
% legend("bottomright",
%        legend=c("Atlas","Atlas/Magma", "Goto", "Goto/Magma", "MKL", "MKL/Magma"),
%        bty="n", col=paircols, lty=1, lwd=3)
% @
% \end{figure}

\subsubsection{Performance of magma and BLAS relative to BLAS}

\begin{figure}[H]
  \centering
<<magmammqr,fig=TRUE,height=5.25,width=11>>=
op <- par(mfrow=c(1,2))
D <- subset(i7[,-c(1:2,5)], type=='matmult')
N <- cbind(D[,"nobs"],
           D[,"atlas"]/D[,"magmaAtlas"],
           D[,"atl39"]/D[,"magmaAtl39"],
           D[,"gotob"]/D[,"magmaGoto"],
           D[,"mkl"]/D[,"magmaMkl"])
matplot(x=D[,"nobs"], y=N[,-1],
        type='l', lty=1, lwd=3, col=cols[-1], #pch=".",
        xlab="Matrix dimension", ylab="Magma performance relative to BLAS",
        main="Matrix Multiplication:\nRatio of BLAS to BLAS+Magma")
legend("topright", legend=c("Atlas","Atl39","Goto", "MKL"),
       bty="n", col=cols[-1], lty=1, lwd=3)

#D <- subset(i7[,-c(1:2,5)], type=='qr')
N <- cbind(D[,"nobs"],
           D[,"atlas"]/D[,"magmaAtlas"],
           D[,"atl39"]/D[,"magmaAtl39"],
           D[,"gotob"]/D[,"magmaGoto"],
           D[,"mkl"]/D[,"magmaMkl"])
matplot(x=D[,"nobs"], y=N[,-1],
        type='n',
        lty=1, lwd=3, col=cols[-1], #pch=".",
        xlab="Matrix dimension", ylab="Magma performance relative to BLAS",
        main="QR Decomposition:\nRatio of BLAS to BLAS+Magma")
legend("bottomright", legend=c("Atlas","Atl39","Goto", "MKL"),
       bty="n", col=cols[-1], lty=1, lwd=3)
@
\caption{Matrix multiplication and QR decomposition with Magma}
\end{figure}

This chart illustrates a significant improvement for matrix multiplication in
the case of the single-threaded Atlas libraries.  As matrix size increases,
the ratio increases and approaches 3.5 indicating a 3.5-times speedup for
standard Atlas when combined with \pkg{magma}.  However, for the
already-multithreaded BLAS, the opposite effect is realized: combined
performance is actually \textsl{slower} in the hybrid case. This result seems
invariant to the choice of multi-threaded BLAS as all three implementations perform similarly
poorly when combined with \pkg{magma}.  The other noteworthy effect is the
variability of results as $N$ approaches 1000.

Attempts to run a QR decomposition resulted in segmentation fault on the card
used in these tests (Quadro FX 4800) once the matrix dimensions exceeded the
(relatively modest) size of 64.\footnote{The same library and operating
  system combination worked correctly on a newer card (Fermi C 1060) for
  Brian Smith (personal communication). However, we were unable to test on
  that platform.}  This has been reported to the \pkg{Magma} development
team; hopefully a future library version will allow these tests to be
performed.

\begin{figure}[H]
  \centering
<<magmasvdlu,fig=TRUE,height=5.25,width=11>>=
op <- par(mfrow=c(1,2))
D <- subset(i7[,-c(1:2,5)], type=='svd')
N <- cbind(D[,"nobs"],
           D[,"atlas"]/D[,"magmaAtlas"],
           D[,"atl39"]/D[,"magmaAtl39"],
           D[,"gotob"]/D[,"magmaGoto"],
           D[,"mkl"]/D[,"magmaMkl"])
matplot(x=D[,"nobs"], y=N[,-1],
        type='l', lty=1, lwd=3, col=cols[-1], #pch=".",
        xlab="Matrix dimension", ylab="Magma performance relative to BLAS",
        main="SVD Decomposition:\nRatio of BLAS to BLAS+Magma")
legend("bottomright", legend=c("Atlas", "Atl39", "Goto", "MKL"),
       bty="n", col=cols[-1], lty=1, lwd=3)

D <- subset(i7[,-c(1:2,5)], type=='lu' & nobs<=2000)
N <- cbind(D[,"nobs"],
           D[,"atlas"]/D[,"magmaAtlas"],
           D[,"atl39"]/D[,"magmaAtl39"],
           D[,"gotob"]/D[,"magmaGoto"],
           D[,"mkl"]/D[,"magmaMkl"])
matplot(x=N[,1], y=N[,-1],
        type='l', lty=1, lwd=3, col=cols[-1], #pch=".",
        xlim=c(min(N[,1]), 4000),  # fill in for missing
        xlab="Matrix dimension", ylab="Magma oerformance relative to BLAS",
        main="LU Decomposition:\nRatio of BLAS to BLAS+Magma")
legend("bottomright", legend=c("Atlas", "Atl39", "Goto", "MKL"),
       bty="n", col=cols[-1], lty=1, lwd=3)
@
\caption{SVD and LU decompositions with Magma}
\end{figure}

For the SVD decomposition, we see strong variability for all four BLAS
libraries tested along with Magma. An initial performance penalty (below
$N=500$) gets outweighed by a performance gain up to $N=1000$. However, once
the matrix dimension increases, performance becomes indistinguishable from the
use of just the accelerated BLAS. We should note this case differs from the
others as \pkg{Magma} has not yet implemented an algorithm for the SVD. In
that sense, this test provides an illustration of the potential cost to naive
users and it is in fact an acceptable result that the ratio generally
approaches one.

For LU decomposition, we experienced similar difficulties as for QR
decomposition once the matrix dimension exceeded 2000.  For smaller matrices,
results were promising for the single-threaded Atlas---similar to the case of matrix
multiplication---and unimpressive for Goto and MKL.  In all cases small sizes
lead to performance penalties. For medium-sized matrices Atlas39 manages a
ratio of just above 1.0 indicating a small gain.  The opposite is true for
Goto and MKL.

\section[Summary]{Summary}

We present benchmarking results comparing five different BLAS implementations
for four different matrix computations on two different hardware platforms.
We find reference BLAS to be dominated in all cases. Single-threaded Atlas
BLAS improves on the reference BLAS but loses to multi-threaded BLAS.

For multi-threaded BLAS we find the Goto BLAS dominate the Intel MKL, with a
single exception of the QR decomposition on the xeon-based system which may
reveal an error. The development verson of Atlas, when compiled in
multi-threaded mode is competetive with both Goto BLAS and the MKL.

GPU computing, when used is isolation, is found to be compelling only for
very large matrix sizes.  However, a hybrid approach as suggested by Magma
\citep{Tomov_et_al:2009,Tomov_et_al:2010} has the potential to dominate the
field.  That said, the current implementation was seen to be not yet ready
for wider use, at least not for the current library version and the hardware
choice at our disposal.

Our benchmarking framework can be employed by others through the \proglang{R}
packaging system which could lead to a wider set of benchmark results. These
results could be helpful for next-generation systems which may need to make
heuristic choices about when to compute on the CPU and when to compute on the
GPU. A strong empirical basis may make these heuristics more robust.

\section*{Computational details}

The results in this paper were obtained using \proglang{R}
\Sexpr{paste(R.Version()[6:7], collapse = ".")} with the packages
\pkg{gputools} \Sexpr{gsub("-", "--", packageDescription("gputools")$Version)},
\pkg{magma} \Sexpr{gsub("-", "--", packageDescription("magma")$Version)},
\pkg{RSQLite} \Sexpr{gsub("-", "--", packageDescription("RSQLite")$Version)},
\pkg{DBI} \Sexpr{gsub("-", "--", packageDescription("DBI")$Version)} and
\pkg{getopt} \Sexpr{gsub("-", "--", packageDescription("getopt")$Version)}.
\proglang{R} itself and all packages used are available from CRAN at
\url{http://CRAN.R-project.org/}.

The following Ubuntu package were used to provide the differnt BLAS
implementations: \pkg{libblas} 1.2-build1, \pkg{liblapack3gf} 3.2.1-2,
\pkg{libatlas3gf-base} 3.6.0-24ubuntu, \pkg{revolution-mkl} 3.0.0-1ubuntu1,
(containing MKL 10.2 dated March 2009) and \pkg{gotoblas2} 1.13-1 which was
built using \pkg{gotoblas2-helper} 0.1-12 by \citet{Nakama:2010}. Apart from
\pkg{gotoblas2-helper} and \pkg{gotoblas2}, all these package are available
via every Ubuntu mirror.

\section*{Disclaimers}

NVidia provided the Quadro FX 4800 GPU card  for evaluation / research
purposes.  REvolution Computing (now Revolution Analytics) employed the
author as a paid consultant for the creation of the \pkg{revolution-mkl}
package and integration of REvolution R into Ubuntu 9.10.

\bibliography{gcbd}

\end{document}

%% TODOs:
%%
%% Allan, Luke:  multithreaded Atlas, and tuned (Allan)
%%
%% Mark:  Please also mention memory as a factor in performance - e.g.,
%%    cutoff point for the gpuLm() command differed between DDR2 and DDR3.
%%
%% Mark:  In the introduction to Magma, it may be worth pointing out that
%%    the software dynamically selects the appropriate combination of hardware
%%    to use, based on characteristics of the data.  This can be inferred
%%    from Figure 1, but is not entirely obvious at first glance.
%%
%% Luke:  On QR you might be explicit which one you are using, LINPACK or
%%    LAPACK.  Default in R is still LINPACK, which only uses level 1 BLAS
%%    so won't see much benefit of fancy BLAS implementations.  LAPACK sees
%%    more benefit I think.
%%
%% Luke:
%%    In your magma comparisons are you comparing single/single or
%%    double/double?  Also I assume in the plots above one means magma is
%%    better (so it soind better on intermediate size matrices?) but that
%%    could be more explicit.
%%
%% Dirk :)
%%    Mention Debian blas convention, /etc/ld.so.conf.d/*conf
%%    Think about table with raw data
%%    Add regressions ...
%%
%% Allan
%%   p. 9 - qr single threaded - your comment got me digging a little more
%%   and, as best as I can read the source, the qr() function calls
%%   .Fortran("dqrdc2", ...) [unless you use the LAPACK=TRUE argument] which
%%   is provided not by BLAS but by R itself in src/appl/dqrdc2.f (not
%%   src/extra/blas !)  This is a Ross Ihaka special; the original is
%%   http://www.netlib.org/linpack/dqrdc.f (i.e. without the 2 at the end).
