
%% use JSS class but for now with nojss option
\documentclass[nojss,article]{jss}
\usepackage{float}

<<echo=FALSE,print=FALSE>>=
options(width=50)
library(gcbd)
gcbd.version <- packageDescription("gcbd")$Version
gcbd.date <- packageDescription("gcbd")$Date
# dput(brewer.pal(5, "Set1"))
cols <- c("#E41A1C", "#377EB8", "#4DAF4A", "#984EA3", "#FF7F00")
# dput(brewer.pal(8,"Paired"))
paircols <- c("#A6CEE3", "#1F78B4", "#B2DF8A", "#33A02C", "#FB9A99", "#E31A1C", "#FDBF6F", "#FF7F00")
# create figures/ if not present
if ( ! (file.exists("figures") && file.info("figures")$isdir) ) dir.create("figures")
@

\author{Dirk Eddelbuettel\\Debian Project} % \And Second Author\\Plus Affiliation}
\title{Benchmarking single- and multi-core BLAS implementations and GPUs for use with \proglang{R}}

%% for pretty printing and a nice hypersummary also set:
\Plainauthor{Dirk Eddelbuettel} % , Second Author} %% comma-separated
\Plaintitle{Benchmarking Single- and Multi-Core BLAS Implementaions and GPUs for Use with R}
\Shorttitle{Benchmarking BLAS and GPUs for Use with R}

%% an abstract and keywords
\Abstract{
  \noindent
  We provide timings for common linear algebra subroutines. Several BLAS
  (Basic Linear Algrebra Subprograms) implementations are compared. The first
  is the unoptimised reference BLAS which provides a baseline to measure
  against. Second is the Atlas tuned BLAS, configured for single-threaded
  mode. Third is the optimised and multi-threaded Goto BLAS. Fourth is the
  multithreaded-BLAS contained in the commercial Intel MKL package. We also
  use two GPU-based implementations: \pkg{gputools} as well as \pkg{magma}.

  Several key computations are compared: matrix multiplication, QR
  decomposition and SVD decomposition.

%  This paper corresponds to gcbd version \Sexpr{gcbd.version}.
}
\Keywords{blas, atlas, goto, mkl, gpu}
\Plainkeywords{blas, atlas, goto, mkl, gpu} %% without formatting
%% at least one keyword must be supplied

%% publication information
%% NOTE: Typically, this can be left commented and will be filled out by the technical editor
%% \Volume{13}
%% \Issue{9}
%% \Month{September}
%% \Year{2004}
%% \Submitdate{2004-09-29}
%% \Acceptdate{2004-09-29}

%% The address of (at least) one author should be given
%% in the following format:
\Address{
  Dirk Eddelbuettel \\
  Debian Project \\
  River Forest, IL, USA\\
  E-mail: \email{edd@debian.org}\\
  URL: \url{http://dirk.eddelbuettel.com}
}
%% It is also possible to add a telephone and fax number
%% before the e-mail in the following format:
%% Telephone: +43/1/31336-5053
%% Fax: +43/1/31336-734

%% for those who use Sweave please include the following line (with % symbols):
%% need no \usepackage{Sweave.sty}

%% end of declarations %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{document}
\SweaveOpts{engine=R,eps=FALSE,echo=FALSE,prefix.string=figures/chart}
%\VignetteIndexEntry{BLAS and GPU Benchmarking}
%\VignetteDepends{gputools,magma
%\VignetteKeywords{blas,gpu,atlas,mkl,goto}
%\VignettePackage{gcbd}

%% include your article here, just as usual
%% Note that you should use the \pkg{}, \proglang{} and \code{} commands.

\section[Introduction]{Introduction}

Analysts are often eager to reap the maximum performance from their computing
platform.  A popular suggestion in recent years has been to consider
\textsl{optimised} (in a sense we will define further below) basic linear
algebra subprograms (BLAS).  Some of these have been included with some
commercial analysis platforms \citep{Moler:2000} for a decade. Some have also
been available for others such as common Linux distributions even longer
\citep{Maguire:1999}. The \proglang{R} \textsl{Installation and
Administration} manual also devotes a detailed discussion to the topic
\citep{RCore:InstAdmin}.

Among the BLAS implementations, several popular choices have emerged. Atlas
(an acronym for \textsl{automatically tuned linear algebra system} is popular
as it has shown very good performance due to the automated and cpu-specific
tuning it offers \citep{Whaley_Dongarra:1999,Whaley_Petitet:2005}. It is also
licensed in such a way that it permits distributors to include it in their
products.  Another popular BLAS implementation is Goto BLAS which is named
after its main developer, Kazushige Goto \citep{Goto_VanDeGeijin:2008}. While
`free to use', its license does not permit redistribution.  Lastly, the Intel
MKL, a commercial product, also includes an optimised BLAS library.

A recent addition to the toolchain of high-performance computing are
graphical processing units (GPUs).  Originally designed for optmised
single-precision arithmetic to accelerate computing as performed by graphics
cards, these devices are increasingly used in numerical analysis.  Earlier
criticism of insufficient floating-point precisions or severe performance
penalties for double-precision calculation are being addressed by the newest
models. Dependence on particular vendors remains a concern which NVidia's
CUDA toolkit currently still the development choice where the newer OpenCL
standard may become a more generic successor. \citet{Brodtkorb_et_al_2010}
provide an excellent recent survey.

But what has been lacking is a comparison of the effective performance of these
alternative.  This paper works towards answering this question.  In the next
section, the technical background is discussed before the implementation of
our approach is outlined. We provide our results in the following
section. A summary concludes.

\section{Background}

Basic Linear Algrebra Subprograms (BLAS) provide an Application Programming
Interface (API) linear algebra.  For a givem task as, say, a multiplication
of two conformant matrices, an interface is described via a function
declaration. The actual implementation becomes interchangeable and can be
supplied by different implementations or algorithms.  This is one of the
fundamental code design features we are using here to benchmark the
difference in performance from different implementations.

A second key aspect is the difference between static and shared linking.  In
static linking, object code is copied from the underlying library and copied
into the resulting executable.  This has several key implications. First, the
executable becomes larger due to the copy of code. Second, it makes it
marginally faster as the library code is present and no additional lookup and
subsequent redirect has to be performed (and the amount of this performance
penalty is the subject of near endless debate). Third, it make the program
more robust as fewer external dependencies are required.  However, this last
point also has a downside: no changes in the underlying library will be
reflected in the binary unless a new build is executed.  Shared library
builds, on the other hand, result in smaller binaries that may run marginally
slower -- but can make use of different libraries without a rebuild.  That
last feature is key here.

Because of both the standardised interface of the BLAS, and the fact that we
have several alternative implementations at our disposal, we can readily
switch between these alternatives. This makes use of a package mechanism
used by the Linux distribution we employ. However, a simpler (yet less
robust) approach would also be available. This technical aspect is discussed
further below.

The first available BLAS implementation stems from the original unoptimised
code on Netlib. On Debian-based systems, it is provided by the (source)
packages \pkg{blas} and \pkg{lapack}. It is commonly referred to as
'reference BLAS' (and we will use 'ref' as a shorthand) as it provides a
refernce implementation.

The second BLAS implementation is provided by Atlas
\citep{Whaley_Dongarra:1999,Whaley_Petitet:2005}. Its names stands for
\textsl{automatically tuned linear algebra software} as it optimises its
performance and parameters (such as cache sizes) during its initial build.
Another notable aspect is its liberal licensing which permits wide
distribution.

The third available BLAS implementation is part of the Intel Math Kernel
Library (MKL). This is a commercial product. However, Ubuntu release 9.10
(``karmic'') contains a set of packages sponsored by Revolution Analytics
which comprises the Intel MKL in a setup directly useable by \proglang{R}. We
will use these packages here as a first set of optimised BLAS.

The fourth BLAS implementation is provided by the Goto BLAS. Upon
registration, these are freely available from the University of Texas, albeit
under license that prohibits redistribution.  This prevents inclusion into
popular Liux distributions.  However, the ISM in Japan provides a `helper
package' which, given the required registration information, downloads and
compiles the Goto BLAS in such a manner that another binary Debian package
results.  This allow to use the package as a third BLAS alternative.

The first \proglang{R} extension for graphics-processing units was provided
by the \pkg{gputools} package \citep{cran:gputools}. It provides a number of
functions which use the GPU instead of the CPU, providing a noticeable
speed-up for `large enough' problems.  Because data has to be transferred
from the CPU to the GPU, a fixed cost in communications has to be borne by
every invocation of the GPU.  For sizeable problems, this cost can be
outweighed by the benefits of the massively parallel GPU computation.
Exactly where the indifference point lies beyond which GPU computing has an
advantage is unfortunately dependent on the particular problem and algorithm
as well as the given hardware and software combination.

A very recent addition to the set of \proglang{R} packages is \pkg{magma}
\citep{cran:magma} which interfaces the library project of the same name
\citep{Tomov_Et_Al:2009}.(We will capitalize the library to distinguish it
from the \proglang{R} package). The stated goal of the Magma project it to
\textsl{develop a dense linear algebra library similar to LAPACK but for
  heterogeneous/hybrid architectures, starting with current "Multicore+GPU"
  systems}.

\section{Implementation}

\subsection{Requirements}

In order to undertake the automated benchmarking, we need to be able to
switch between different implementations of the BLAS API.  As discussed
above, dynamic libraries are one possible solution that avoids having to
rebuild \proglang{R} explicitly for each library.  However, this also
requires that \proglang{R} itself is built with shared-library support, as
well as with support for external BLAS and LAPACK libraries.  This is the
default on Debian and Ubuntu system.

The reference BLAS as well as Atlas have been available for essentially all
Debian and Ubuntu releases.  The Intel MKL is available for Ubuntu following
the upload for release 9.10.

For Goto BLAS, we are making use of a helper script provided in the
contributed \texttt{gotoblas2-helper} package \citep{Nakama:2010}. This
package arranges for a download of the Goto BLAS sources (provided the
required account and password information for the University of Texas
software download center) and automated Debian package build and installation
via the command \texttt{sudo /etc/init.d/gotoblas2-helper start}.  While
designed for Debian, it also works perfectly on our Ubuntu
systems.\footnote{The \texttt{init.d} script required one change from
  \texttt{/bin/sh} to   \texttt{/bin/bash}.}

\subsection{Benchmark Implementation}

The bechmarks described in this paper are produced by the package \pkg{gcbd}
which is part of the larger \pkg{gcb} package on R-Forge. The \pkg{gcbd}
package contains a number of \proglang{R} helper functions as well as an
actual benchmarking script which is executed.

The helper functions fall into two groups: utilities, and benchmarks.

\subsection{Hardware consideration}

Mention hardware impact, show baseline results on different platforms?

\pagebreak
\section[Results]{Results}

<<print=FALSE>>=
dbcon <- dbConnect(dbDriver("SQLite"), dbname=system.file("sql", "gcbd.sqlite", package="gcbd"))
i7 <- dbGetQuery(dbcon, 'select * from benchmark where host="max" order by nobs')
xeon <- dbGetQuery(dbcon, 'select * from benchmark where host="xeon_X5570" order by nobs')
invisible(dbDisconnect(dbcon))
D <- subset(i7[,-c(1:2,5)], type=="matmult")
@

\subsection{BLAS Comparison}

\subsubsection{Matrix Multiplication: i7 with GPU}

\setkeys{Gin}{width=0.99\textwidth}
\begin{figure}[H]
  \centering
<<fig=TRUE,height=6,width=12>>=
D <- subset(i7[,-c(1:2,5)], type=='matmult')
op <- par(mfrow=c(1,2))
matplot(x=D[,"nobs"], y=D[,c("ref","atlas","mkl","gotob","gpu")], type='l', lty=1, col=cols,
        xlab="Matrix dimension", ylab="Time in seconds", main="Matrix Multiplication")
legend("topleft", legend=c("Ref","Atlas","MKL","Goto","GPU"), bty="n", col=cols, lty=1)
matplot(x=D[,"nobs"], y=D[,c("ref","atlas","mkl","gotob","gpu")], type='l', lty=1, col=cols,
        xlab="Matrix dimension", ylab="Time in seconds (in logs)", main="Matrix Multiplication", log="y")
legend("bottomright", legend=c("Ref","Atlas","MKL","Goto","GPU"), bty="n", col=cols, lty=1)
par(op)
@
\end{figure}

\subsubsection{Matrix Multiplication: Xeon}

\begin{figure}[H]
  \centering
<<fig=TRUE,height=6,width=12>>=
D <- subset(xeon[,-c(1:2,5)], type=='matmult')
op <- par(mfrow=c(1,2))
matplot(x=D[,"nobs"], y=D[,c("ref","atlas","mkl","gotob")], type='l', lty=1, col=cols,
        xlab="Matrix dimension", ylab="Time in seconds", main="Matrix Multiplication")
legend("topleft", legend=c("Ref","Atlas","MKL","Goto"), bty="n", col=cols, lty=1)
matplot(x=D[,"nobs"], y=D[,c("ref","atlas","mkl","gotob")], type='l', lty=1, col=cols,
        xlab="Matrix dimension", ylab="Time in seconds (in logs)", main="Matrix Multiplication", log="y")
legend("bottomright", legend=c("Ref","Atlas","MKL","Goto"), bty="n", col=cols, lty=1)
par(op)
@
\end{figure}


\subsubsection{QR Decomposition: i7 with GPU}

\begin{figure}[H]
  \centering
<<fig=TRUE,height=6,width=12>>=
D <- subset(i7[,-c(1:2,5)], type=='qr')
op <- par(mfrow=c(1,2))
matplot(x=D[,"nobs"], y=D[,c("ref","atlas","mkl","gotob","gpu")], type='l', lty=1, col=cols,
        xlab="Matrix dimension", ylab="Time in seconds", main="QR Decomposition")
legend("topleft", legend=c("Ref","Atlas","MKL","Goto","GPU"), bty="n", col=cols, lty=1)
matplot(x=D[,"nobs"], y=D[,c("ref","atlas","mkl","gotob","gpu")], type='l', lty=1, col=cols, log="y",
        xlab="Matrix dimension", ylab="Time in seconds (in logs)", main="QR Decomposition")
legend("bottomright", legend=c("Ref","Atlas","MKL","Goto","GPU"), bty="n", col=cols, lty=1)
par(op)
@
\end{figure}

\subsubsection{QR Decomposition: Xeon}

\begin{figure}[H]
  \centering
<<fig=TRUE,height=6,width=12>>=
D <- subset(xeon[,-c(1:2,5)], type=='qr')
op <- par(mfrow=c(1,2))
matplot(x=D[,"nobs"], y=D[,c("ref","atlas","mkl","gotob")], type='l', lty=1, col=cols,
        xlab="Matrix dimension", ylab="Time in seconds", main="QR Decomposition")
legend("topleft", legend=c("Ref","Atlas","MKL","Goto"), bty="n", col=cols, lty=1)
matplot(x=D[,"nobs"], y=D[,c("ref","atlas","mkl","gotob")], type='l', lty=1, col=cols,
        xlab="Matrix dimension", ylab="Time in seconds (in logs)", main="QR Decomposition", log="y")
legend("bottomright", legend=c("Ref","Atlas","MKL","Goto"), bty="n", col=cols, lty=1)
par(op)
@
\end{figure}

\subsubsection{SVD Decomposition: i7 with GPU}

\begin{figure}[H]
  \centering
<<fig=TRUE,height=6,width=12>>=
D <- subset(i7[,-c(1:2,5)], type=='svd')
op <- par(mfrow=c(1,2))
matplot(x=D[,"nobs"], y=D[,c("ref","atlas","mkl","gotob","gpu")], type='l', lty=1, col=cols,
        xlab="Matrix dimension", ylab="Time in seconds", main="SVD Decomposition")
legend("topleft", legend=c("Ref","Atlas","MKL","Goto","GPU"), bty="n", col=cols, lty=1)
matplot(x=D[,"nobs"], y=D[,c("ref","atlas","mkl","gotob","gpu")], type='l', lty=1, col=cols, log="y",
        xlab="Matrix dimension", ylab="Time in seconds (in logs)", main="SVD Decomposition")
legend("bottomright", legend=c("Ref","Atlas","MKL","Goto","GPU"), bty="n", col=cols, lty=1)
par(op)
@
\end{figure}

\subsubsection{SVD Decomposition: Xeon}

\begin{figure}[H]
  \centering
<<fig=TRUE,height=6,width=12>>=
D <- subset(xeon[,-c(1:2,5)], type=='svd')
op <- par(mfrow=c(1,2))
matplot(x=D[,"nobs"], y=D[,c("ref","atlas","mkl","gotob")], type='l', lty=1, col=cols,
        xlab="Matrix dimension", ylab="Time in seconds", main="SVD Decomposition")
legend("topleft", legend=c("Ref","Atlas","MKL","Goto"), bty="n", col=cols, lty=1)
matplot(x=D[,"nobs"], y=D[,c("ref","atlas","mkl","gotob")], type='l', lty=1, col=cols,
        xlab="Matrix dimension", ylab="Time in seconds (in logs)", main="SVD Decomposition", log="y")
legend("bottomright", legend=c("Ref","Atlas","MKL","Goto"), bty="n", col=cols, lty=1)
par(op)
@
\end{figure}


\subsection{Magma: GPU and BLAS combined}

\begin{figure}[H]
  \centering
<<fig=TRUE,height=6,width=12>>=
D <- subset(i7[,-c(1:2,5)], type=='matmult')
op <- par(mfrow=c(1,2))
matplot(x=D[,"nobs"], y=D[,c("atlas","magmaAtlas","gotob","magmaGoto", "mkl", "magmaMkl")],
        type='l', lty=1, col=paircols,
        xlab="Matrix dimension", ylab="Time in seconds", main="Matrix Multiplication")
legend("topleft", legend=c("Atlas","Atlas/Magma", "Goto", "Goto/Magma", "MKL", "MKL/Magma"),
       bty="n", col=paircols, lty=1)
matplot(x=D[,"nobs"], y=D[,c("atlas","magmaAtlas","gotob","magmaGoto", "mkl", "magmaMkl")],
        type='l', lty=1, col=paircols, log="y",
        xlab="Matrix dimension", ylab="Time in seconds (in logs)", main="Matrix Multiplication")
legend("bottomright",
       legend=c("Atlas","Atlas/Magma", "Goto", "Goto/Magma", "MKL", "MKL/Magma"),
       bty="n", col=paircols, lty=1)
par(op)
@
\end{figure}

\begin{figure}[H]
  \centering
<<fig=TRUE,height=6,width=12>>=
D <- subset(i7[,-c(1:2,5)], type=='qr')
op <- par(mfrow=c(1,2))
matplot(x=D[,"nobs"], y=D[,c("atlas","magmaAtlas","gotob","magmaGoto", "mkl", "magmaMkl")],
        type='l', lty=1, col=paircols,
        xlab="Matrix dimension", ylab="Time in seconds", main="QR Decomposition")
legend("topleft", legend=c("Atlas","Atlas/Magma", "Goto", "Goto/Magma", "MKL", "MKL/Magma"),
       bty="n", col=paircols, lty=1)
matplot(x=D[,"nobs"], y=D[,c("atlas","magmaAtlas","gotob","magmaGoto", "mkl", "magmaMkl")],
        type='l', lty=1, col=paircols, log="y",
        xlab="Matrix dimension", ylab="Time in seconds (in logs)", main="QR Decomposition")
legend("bottomright",
       legend=c("Atlas","Atlas/Magma", "Goto", "Goto/Magma", "MKL", "MKL/Magma"),
       bty="n", col=paircols, lty=1)
par(op)
@
\end{figure}

\begin{figure}[H]
  \centering
<<fig=TRUE,height=6,width=12>>=
D <- subset(i7[,-c(1:2,5)], type=='svd')
op <- par(mfrow=c(1,2))
matplot(x=D[,"nobs"], y=D[,c("atlas","magmaAtlas","gotob","magmaGoto", "mkl", "magmaMkl")],
        type='l', lty=1, col=paircols,
        xlab="Matrix dimension", ylab="Time in seconds", main="SVD Decomposition")
legend("topleft", legend=c("Atlas","Atlas/Magma", "Goto", "Goto/Magma", "MKL", "MKL/Magma"),
       bty="n", col=paircols, lty=1)
matplot(x=D[,"nobs"], y=D[,c("atlas","magmaAtlas","gotob","magmaGoto", "mkl", "magmaMkl")],
        type='l', lty=1, col=paircols, log="y",
        xlab="Matrix dimension", ylab="Time in seconds (in logs)", main="SVD Decomposition")
legend("bottomright",
       legend=c("Atlas","Atlas/Magma", "Goto", "Goto/Magma", "MKL", "MKL/Magma"),
       bty="n", col=paircols, lty=1)
par(op)
@
\end{figure}


\begin{figure}[H]
  \centering
<<fig=TRUE,height=6,width=12>>=
op <- par(mfrow=c(1,3))
D <- subset(i7[,-c(1:2,5)], type=='matmult')
N <- cbind(D[,"nobs"],
           D[,"ref"]/D[,"atlas"],
           D[,"ref"]/D[,"magmaAtlas"],
           D[,"ref"]/D[,"gotob"],
           D[,"ref"]/D[,"magmaGoto"],
           D[,"ref"]/D[,"mkl"],
           D[,"ref"]/D[,"magmaMkl"],
           D[,"ref"]/D[,"gpu"])
matplot(x=D[,"nobs"], y=N[,-1],
        type='l', lty=1, col=paircols, #pch=".",
        xlab="Matrix dimension", ylab="Performance relative to Ref.BLAS",
        main="Matrix Multiplication:\nRatio of Reference BLAS to given BLAS")
legend("bottomright",
       legend=c("Atlas","Atlas/Magma", "Goto", "Goto/Magma", "MKL", "MKL/Magma", "GPU"),
       bty="n", col=paircols, lty=1)

D <- subset(i7[,-c(1:2,5)], type=='qr')
N <- cbind(D[,"nobs"],
           D[,"ref"]/D[,"atlas"],
           D[,"ref"]/D[,"magmaAtlas"],
           D[,"ref"]/D[,"gotob"],
           D[,"ref"]/D[,"magmaGoto"],
           D[,"ref"]/D[,"mkl"],
           D[,"ref"]/D[,"magmaMkl"],
           D[,"ref"]/D[,"gpu"])
matplot(x=D[,"nobs"], y=N[,-1],
        type='l', lty=1, col=paircols, #pch=".",
        xlab="Matrix dimension", ylab="Performance relative to Ref.BLAS",
        main="QR Decomposition:\nRatio of Reference BLAS to given BLAS")
legend("bottomright",
       legend=c("Atlas","Atlas/Magma", "Goto", "Goto/Magma", "MKL", "MKL/Magma", "GPU"),
       bty="n", col=paircols, lty=1)

D <- subset(i7[,-c(1:2,5)], type=='svd')
N <- cbind(D[,"nobs"],
           D[,"ref"]/D[,"atlas"],
           D[,"ref"]/D[,"magmaAtlas"],
           D[,"ref"]/D[,"gotob"],
           D[,"ref"]/D[,"magmaGoto"],
           D[,"ref"]/D[,"mkl"],
           D[,"ref"]/D[,"magmaMkl"],
           D[,"ref"]/D[,"gpu"])
matplot(x=D[,"nobs"], y=N[,-1],
        type='l', lty=1, col=paircols, #pch=".",
        xlab="Matrix dimension", ylab="Performance relative to Ref.BLAS",
        main="SVD Decomposition:\nRatio of Reference BLAS to given BLAS")
legend("bottomright",
       legend=c("Atlas","Atlas/Magma", "Goto", "Goto/Magma", "MKL", "MKL/Magma", "GPU"),
       bty="n", col=paircols, lty=1)

@
\end{figure}

\section[Summary]{Summary}

TBD

\bibliography{gcbd}

\end{document}
